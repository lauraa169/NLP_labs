{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP 2026\n",
    "# Lab 2: Word Vectors and Information Retrieval\n",
    "## *alt*-title: üöÄ Project CleanSearch AI, a DOGE initiative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## üèõÔ∏èüêï PRESS RELEASE ‚Äî For Immediate (and Maximum Efficiency) Distribution  \n",
    "\n",
    "### The Department of Outdated Government Encyclopedias (DOGE) Launches Revolutionary NLP Project to Rescue Public Knowledge  \n",
    "\n",
    "**Washington, D.C.** ‚Äî In a bold step toward modernizing the nation‚Äôs most chaotic digital archives, the   **Department of Outdated Government Encyclopedias (DOGE)** today announced the launch of its new initiative:  üöÄ **Project CleanSearch AI**.\n",
    "\n",
    "For decades, citizens have struggled to find simple answers hidden inside massive, noisy, and poorly structured government knowledge repositories.\n",
    "\n",
    "Questions such as:\n",
    "\n",
    "- ‚ÄúWho won the Nobel Prize in 1930?‚Äù  \n",
    "- ‚ÄúWhen did Angola become independent?‚Äù  \n",
    "\n",
    "have resulted in thousands of irrelevant web pages, confusing biographies and excessive scrolling üìâ\n",
    "\n",
    "> *‚ÄúFrankly, our archives are a mess,‚Äù* said a DOGE spokesperson.  \n",
    "> *‚ÄúThey‚Äôre long, noisy and about as searchable as a pile of printed Wikipedia pages thrown into a hurricane.‚Äù*\n",
    "\n",
    "### üß† The Solution  \n",
    "\n",
    "DOGE has assembled an elite team of AI specialists, hired from UM DACS 2nd year bachelor program with the following goals:\n",
    "\n",
    "‚úÖ Clean decades of messy digital text  \n",
    "‚úÖ Extract meaningful knowledge  \n",
    "‚úÖ Replace outdated keyword search with modern **retrieval systems**  \n",
    "‚úÖ Deliver instant, accurate answers to citizens  \n",
    "\n",
    "Using real-world noisy data similar to the government‚Äôs archives, the team will experiment with multiple retrieval models to determine the most efficient approach, methods which have been taught in the fabulous classes of some person quoted as J.S. \n",
    "\n",
    "Whispers across the digital corridors suggest that DOGE may soon supercede the legendary Project 2-2, though DACS management insist these rumours are ‚Äúunder control.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deliverable:\n",
    "\n",
    "- You are asked to deliver **two files only**:\n",
    "  - your executed notebook file (`.ipynb`), and\n",
    "  - your poster (`.pdf`).  \n",
    "  No other files will be taken into consideration.\n",
    "  \n",
    "‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è Each part of the poster will contribute to your grade proportionally to what we present below. If we can't find the relevant part in your notebook (e.g. the figure or the code to support your findings) we will reduce (or even zero-out) your grade for that part.\n",
    "\n",
    "### Instructions for the poster: \n",
    "\n",
    "The final deliverable for this lab is a **scientific poster** presenting your work on building and evaluating a sentence retrieval system using the TriviaQA dataset.\n",
    "- üìè **Size:** A0 or A1  \n",
    "- üß≠ **Orientation:** Portrait or landscape (your choice)  \n",
    "- üìë **Layout:** Clear section structure (e.g., columns or blocks)  \n",
    "\n",
    "#### Your poster should include the following sections:\n",
    "---\n",
    "#### 1Ô∏è‚É£ Problem & Motivation üéØ\n",
    "- Describe the retrieval task (query ‚Üí correct answer document) and the challenges\n",
    "- Briefly introduce the dataset and its challenges  \n",
    "#### 2Ô∏è‚É£ Data Preparation üßπ\n",
    "Explain:\n",
    "- Train / validation / test splitting  \n",
    "- Your cleaning pipeline (at least 6 preprocessing steps)  \n",
    "Include at least one **before vs after cleaning** example.\n",
    "#### 3Ô∏è‚É£ Retrieval Models ü§ñ\n",
    "Present and explain the modes you used:\n",
    "- Bag-of-Words + cosine similarity  \n",
    "- TF-IDF + cosine similarity  \n",
    "- Sentence embeddings (averaged word embeddings)  \n",
    "- [any other model?] \n",
    "Discuss strengths and limitations of each.\n",
    "#### 4Ô∏è‚É£ Qualitative Analysis üîç\n",
    "Provide:\n",
    "- At least **3 successful retrieval examples**  \n",
    "- At least **3 failure cases**  \n",
    "Explain why each worked or failed.\n",
    "#### 5Ô∏è‚É£ Quantitative Evaluation üìä (Main focus)\n",
    "Report **Recall@K** (and possibly other metrics) on the **test set** for all methods:\n",
    "- BOW  \n",
    "- TF-IDF  \n",
    "- Pre-trained embeddings  \n",
    "- [Additional models]  \n",
    "Include relevant table(s) and/or plot(s) and briefly discuss trends.\n",
    "#### 6Ô∏è‚É£ Discussion & Recommendations üí°\n",
    "Conclude with:\n",
    "- Which method you would recommend and why  \n",
    "- Key tradeoffs  \n",
    "- Possible improvements  \n",
    "### üé® Optional Creative Element (Bonus)\n",
    "\n",
    "You may (optionally) present your poster within the fictional storyline of üèõÔ∏è **DOGE ‚Äî Department of Outdated Government Encyclopedias**, where your retrieval system modernizes chaotic national archives and replaces legacy keyword search. Creativity is welcome, but scientific clarity is the priority. We will vote for the \"most creative poster\".\n",
    "\n",
    "---\n",
    "\n",
    "### üìè Evaluation Focus\n",
    "\n",
    "Posters will be assessed on:\n",
    "- Correctness of the pipeline incl. the code (25%)\n",
    "- Clarity of explanations and interpretations of results (25%)\n",
    "- Quality of analysis (20%)\n",
    "- Proper use of evaluation metrics (e.g. Recall@K) (10%)\n",
    "- Visual organization (10%)\n",
    "- Discussion and recommendations (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparing the dataset\n",
    "\n",
    "As in the last lab, we will be using huggingface datasets library ([https://huggingface.co/datasets](https://huggingface.co/datasets)). We will work with TriviaQA dataset ([https://huggingface.co/datasets/sentence-transformers/trivia-qa](https://huggingface.co/datasets/sentence-transformers/trivia-qa)), which contains pairs of queries and articles that contain the answer.\n",
    "\n",
    "In this section we will prepare the dataset, aka clean the sentences and tokenize. We will additionally extract the answers, as some articles correspond to multiple queries. We will create a separate dataset from the unique answers. We will do that for each split separately, so that we can test our retrieval fairly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's start with importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:20.814984679Z",
     "start_time": "2026-02-11T12:18:18.456562093Z"
    }
   },
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from datasets import DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/Desktop/uni/NLP labs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/laura/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/laura/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading\n",
    "Now, we can begin loading the dataset and inspecting the fields."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:23.137265299Z",
     "start_time": "2026-02-11T12:18:20.899258363Z"
    }
   },
   "source": [
    "dataset = datasets.load_dataset('sentence-transformers/trivia-qa')\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 73346\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:23.242023683Z",
     "start_time": "2026-02-11T12:18:23.167485595Z"
    }
   },
   "source": [
    "for i in range(5):\n",
    "    print(dataset['train'][i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?', 'answer': 'The Nobel Prize in Literature 1930 The Nobel Prize in Literature 1930 Sinclair Lewis The Nobel Prize in Literature 1930 Sinclair Lewis Prize share: 1/1 The Nobel Prize in Literature 1930 was awarded to Sinclair Lewis \"for his vigorous and graphic art of description and his ability to create, with wit and humour, new types of characters\". Photos: Copyright ¬© The Nobel Foundation Share this: To cite this page MLA style: \"The Nobel Prize in Literature 1930\". Nobelprize.org. Nobel Media AB 2014. Web. 18 Jan 2017. <http://www.nobelprize.org/nobel_prizes/literature/laureates/1930/>'}\n",
      "{'query': 'Where in England was Dame Judi Dench born?', 'answer': 'Judi Dench - IMDb IMDb Actress | Music Department | Soundtrack Judi Dench was born in York, England, to Eleanora Olive (Jones), who was from Dublin, Ireland, and Reginald Arthur Dench, a doctor from Dorset, England. She attended Mount School in York, and studied at the Central School of Speech and Drama. She has performed with Royal Shakespeare Company, the National Theatre, and at Old Vic Theatre. She is a ... See full bio ¬ª Born: a list of 35 people created 02\\xa0Jul\\xa02011 a list of 35 people created 19\\xa0Apr\\xa02012 a list of 35 people created 28\\xa0May\\xa02014 a list of 25 people created 05\\xa0Aug\\xa02014 a list of 26 people created 18\\xa0May\\xa02015 Do you have a demo reel? Add it to your IMDbPage How much of Judi Dench\\'s work have you seen? User Polls Won     1     Oscar. Another    59 wins & 163 nominations. See more awards \\xa0¬ª Known For \\xa02016 The Hollow Crown (TV Series) Cecily, Duchess of York \\xa02015 The Vote (TV Movie) Christine Metcalfe - Total War (1996) ... Narrator (voice) - Stalemate (1996) ... Narrator (voice) \\xa01992 The Torch (TV Mini-Series) Aba \\xa01990 Screen One (TV Series) Anne \\xa01989 Behaving Badly (TV Mini-Series) Bridget \\xa01981 BBC2 Playhouse (TV Series) Sister Scarli \\xa01976 Arena (TV Series documentary) Sweetie Simpkins \\xa01973 Ooh La La! (TV Series) Am√©lie \\xa01966 Court Martial (TV Series) Marthe \\xa01963 Z Cars (TV Series) Elena Collins \\xa01963 Love Story (TV Series) Pat McKendrick \\xa01960 The Terrible Choice (TV Series) Good Angel Music department (1 credit) \\xa0 A Fine Romance (TV Series) (theme sung by - 14 episodes, 1981 - 1983) (theme song sung by - 12 episodes, 1983 - 1984) - A Romantic Meal (1984) ... (theme song sung by) - Problems (1984) ... (theme song sung by) \\xa02013 Fifty Years on Stage (TV Movie) (performer: \"Send in the Clowns\") \\xa02009 Nine (performer: \"Folies Berg√®re\") - What\\'s Wrong with Mrs Bale? (1997) ... (performer: \"Raindrops Keep Fallin\\' On My Head\" - uncredited) - Misunderstandings (1993) ... (performer: \"Walkin\\' My Baby Back Home\" - uncredited) \\xa01982-1984 A Fine Romance (TV Series) (performer - 2 episodes) - The Telephone Call (1984) ... (performer: \"Boogie Woogie Bugle Boy\" - uncredited) - Furniture (1982) ... (performer: \"Rule, Britannia!\" - uncredited) Hide\\xa0 \\xa02009 Waiting in Rhyme (Video short) (special thanks) \\xa02007 Expresso (Short) (special thanks) \\xa01999 Shakespeare in Love and on Film (TV Movie documentary) (thanks - as Dame Judi Dench) Hide\\xa0 \\xa02016 Rio Olympics (TV Mini-Series) Herself \\xa02015 In Conversation (TV Series documentary) Herself \\xa02015 Entertainment Tonight (TV Series) Herself \\xa02015 CBS This Morning (TV Series) Herself - Guest \\xa02015 The Insider (TV Series) Herself \\xa01999-2014 Cinema 3 (TV Series) Herself \\xa02013 Good Day L.A. (TV Series) Herself - Guest \\xa02013 Arena (TV Series documentary) Herself \\xa02013 At the Movies (TV Series) Herself \\xa02013 Shooting Bond (Video documentary) Herself \\xa02013 Bond\\'s Greatest Moments (TV Movie documentary) Herself \\xa02012 Made in Hollywood (TV Series) Herself \\xa01999-2012 Charlie Rose (TV Series) Herself - Guest \\xa02008-2012 This Morning (TV Series) Herself - Guest \\xa02012 The Secrets of Skyfall (TV Short documentary) Herself \\xa02012 Anderson Live (TV Series) Herself \\xa02012 J. Edgar: A Complicated Man (Video documentary short) Herself \\xa02011 The Many Faces of... (TV Series documentary) Herself / Various Characters \\xa02011 Na plov√°rne (TV Series) Herself \\xa02010 BBC Proms (TV Series) Herself \\xa02010 The South Bank Show Revisited (TV Series documentary) Herself - Episode #6.68 (2009) ... Herself - Guest (as Dame Judi Dench) \\xa02007-2009 Breakfast (TV Series) \\xa02009 Larry King Live (TV Series) Herself - Guest \\xa02009 The One Show (TV Series) Herself \\xa02009 Cranford in Detail (Video documentary short) Herself / Miss Matty Jenkins (as Dame Judi Dench) \\xa02005-2008 The South Bank Show (TV Series documentary) Herself \\xa02008 Tavis Smiley (TV Series) Herself - Guest \\xa02007 ITV News (TV Series) Herself - BAFTA Nominee \\xa02007 The Making of Cranford (Video documentary short) Herself / Miss Matty Jenkyns (as Dame Judi Dench) \\xa02006 Becoming Bond (TV Movie documentary) Herself \\xa02006 Coraz√≥n de... (TV Series) Hers'}\n",
      "{'query': 'In which decade did Billboard magazine first publish and American hit chart?', 'answer': 'The US Billboard song chart The US Billboard song chart Search this site with Google Song chart US Billboard The Billboard magazine has published various music charts starting (with sheet music) in 1894, the first \"Music Hit Parade\" was published in 1936 , the first \"Music Popularity Chart\" was calculated in 1940 . These charts became less irregular until the weekly \"Hot 100\" was started in 1958 . The current chart combines sales, airplay and downloads. A music collector that calls himself Bullfrog has been consolidating the complete chart from 1894 to the present day.  he has published this information in a comprehenive spreadsheet (which can be obtained at bullfrogspond.com/ ). The Bullfrog data assigns each song a unique identifier, something like \"1968_076\" (which just happens to be the Bee Gees song \"I\\'ve Gotta Get A Message To You\"). This \"Whitburn Number\" is provided to match with the books of Joel Whitburn and consists of the year and a ranking within the year. A song that first entered the charts in December and has a long run is listed the following year. This numbering scheme means that songs which are still in the charts cannot be assigned a final id, because their ranking might change. So the definitive listing for a year cannot be final until about April. In our listing we only use songs with finalised IDs, this means that every year we have to wait until last year\\'s entries are finalised before using them. (Source bullfrogspond.com/ , the original version used here was 20090808 with extra data from: the 2009 data from 20091219 the 2010 data from 20110305 the 2011 data from 20120929 the 2012 data from 20130330 the 2013 data from 20150328 The 20150328 data was the last one produced before the Billboard company forced the data to be withdrawn. As far as we know there are no more recent data sets available. This pattern of obtaining the data for a particular year in the middle of the following one comes from the way that the Bullfrog project generates the identifier for a song (what they call the \"Prefix\" in the spreadsheet). Recent entries are identified with keys like \"2015-008\" while older ones have keys like \"2013_177\". In the second case the underscore is significant, it indicates that this was the 177th biggest song released in 2013. Now, of course, during the year no one knows where a particular song will rank, so the underscore names can\\'t be assigned until every song from a particular year has dropped out of the charts, so recent records are temporarily assigned a name with a dash. In about May of the following year the rankings are calculated and the final identifiers are assigned. That is why we at the Turret can only grab this data retrospectively. Attributes The original spreadsheet has a number of attributes, we have limited our attention to just a few of them: 134 9 The songs with the most entries on the chart were White Christmas (with 33 versions and a total of 110 weeks) and Stardust (with 19 and a total of 106 weeks). position The peak position that songs reached in the charts should show an smooth curve from number one down to the lowest position. This chart has more songs in the lower peak positions than one would expect. Before 1991 the profile of peak positions was exactly as you would expect, that year Billboard introduced the concept of \"Recurrent\" tracks, that is they removed any track from the chart which had spent more than twenty weeks in the chart and had fallen to the lower positions. weeks The effect of the \"Recurrent\" process, by which tracks are removed if they have spent at least twenty weeks in the chart and have fallen to the lower reaches, can clearly be seen in the strange spike in this attribute. This \"adjustment\" was intended to promote newer songs and ensure the chart does not become \"stale\". In fact since it was introduced in 1991 the length of long chart runs has increased, this might reflect the more conscious efforts of record companies to \"game\" the charts by controlling release times and promotions, or it coul'}\n",
      "{'query': 'From which country did Angola achieve independence in 1975?', 'answer': \"Angola from past to present | Conciliation Resources Angola from past to present Angola from past to present From military peace to social justice? The Angolan peace process Publication date:\\xa0 David Birmingham When Angola achieved independence in 1975, a war was raging between competing national liberation movements and their foreign backers. Guus Meijer and David Birmingham revisit Angola‚Äôs colonial period and the independence struggle that followed and ask how the resulting social and economic divisions shaped and were manipulated by the warring parties. The article describes the introduction of authoritarian one-party rule under the MPLA and the impact of natural resource development and international and regional powers on the conflict. Tracing the conflict up to the signing of the Luena Memorandum, the authors conclude that Angola‚Äôs peace remains incomplete and that the country faces many challenges in achieving social and democratic reconstruction. Read full article Angola from past to present On 11 November 1975, the Popular Movement for the Liberation of Angola (MPLA) declared Angola's independence and installed Agostinho Neto as its first President in the former Portuguese colony's capital at Luanda. This outcome had long seemed uncertain and indeed even unlikely; the MPLA had not only had to deal with its own serious internal troubles and disaffections, but had also had to take on the Portuguese colonial army and the two rival armed movements, each backed by powerful allies. Holden Roberto's National Front for the Liberation of Angola (FNLA) had initially been the most powerful of the three competing national liberation movements and in the autumn of 1975 it came close to capturing Luanda from the north, backed by a heavily armed force supplied by President Mobuto Sese Seko of Zaire (now the Democratic Republic of Congo). In the south, two armoured columns of a South African invasion force, acting in military coordination with the Union for the Total Independence of Angola (UNITA), led by Jonas Savimbi, almost reached Luanda before they were stopped by Cuban troops which had been rushed to the assistance of the MPLA. The independent Angolan state was thus born out of turmoil and violence and amid serious national, regional and global rivalries. This heritage with its deep historical roots was to influence the unfolding of events for a long time. Angola, like most African countries, grew out of a conglomerate of peoples and groups each with its own distinct history and traditions. Gradually small local nations and states came into contact with each other and historical developments drove them to share a common destiny under increasing Portuguese influence. Long before the arrival of the Portuguese, Bantu-speaking communities had established a farming economy over most of the territory. They had absorbed many of the scattered Khoisan-speaking populations and developed a successful pastoral dimension to their agriculture as well as building up trading economies. One of the most successfully diverse market centres became the town of M'banza Kongo around which the Kongo kingdom evolved. Further east the concept of state formation related to the political ideology of the Lunda peoples while in the south later kingdoms took shape in the highlands of the Ovimbundu people. Angola under Portuguese rule Although the first Portuguese traders, explorers and soldiers set foot on this part of the African coast from 1483, modern colonisation of the whole territory was only formalised four centuries later after the Berlin Conference of 1884-85. Wide stretches of Angola experienced colonial rule for less than a century, and even after 1900 armed revolts broke out and resistance movements sprang up as among the Ovimbundu and the Bakongo from 1913, until the last northern resistance was put down in 1917. During its century of overrule the colonial regime left crucial marks on Angolan society. Its discriminatory legislation, particularly the Statute of the Portuguese Natives of the Provinces of Angola, Mozambique, and Guinea, separ\"}\n",
      "{'query': 'Which city does David Soul come from?', 'answer': 'David Soul - IMDb IMDb Actor | Soundtrack | Director David Soul achieved pop icon status as handsome, blond-haired, blue-eyed Detective Kenneth Hutchinson on the cult \"buddy cop\" TV series Starsky and Hutch (1975), Soul also had a very successful singing career recording several albums, with worldwide number one hit singles including \"Silver Lady\" & \"Don\\'t Give Up on Us Baby\". Born in Chicago, ... See full bio ¬ª Born: Share this page: Related News a list of 43 people created 14\\xa0Jan\\xa02011 a list of 37 people created 13\\xa0Mar\\xa02011 a list of 48 people created 26\\xa0Mar\\xa02012 a list of 973 people created 26\\xa0Feb\\xa02013 a list of 127 people created 05\\xa0Jul\\xa02014 Do you have a demo reel? Add it to your IMDbPage How much of David Soul\\'s work have you seen? User Polls 1 win & 3 nominations. See more awards \\xa0¬ª Known For Starsky and Hutch Det. Ken \\'Hutch\\' Hutchinson (1975-1979) \\xa02004 The Dark Lantern (TV Movie) Storyteller \\xa02004 Dalziel and Pascoe (TV Series) Detective Gus D\\'Amato \\xa01995 Vents contraires (TV Movie) Quill \\xa01994 High Tide (TV Series) Brian Landis \\xa01991-1993 Murder, She Wrote (TV Series) Jordan Barnett / Wes McSorley \\xa01990 The Young Riders (TV Series) Jeremy Styles \\xa01989 Prime Target (TV Movie) Peter Armetage \\xa01989 Deadly Nightmares (TV Series) Cooper Halliday \\xa01989 Alfred Hitchcock Presents (TV Series) Michael Dennison \\xa01987 Crime Story (TV Series) Dr. Newhouse \\xa01987 Harry\\'s Hong Kong (TV Movie) Harry Petros \\xa01986 The Fifth Missile (TV Movie) Capt. Kevin Harris \\xa01984 Partners in Crime (TV Series) Harry \\xa01983 Through Naked Eyes (TV Movie) William Parrish \\xa01982 World War III (TV Movie) Col. Jake Caffey \\xa01980 Homeward Bound (TV Movie) Jake Seaton \\xa01980 Swan Song (TV Movie) Jesse Swan \\xa01974 Medical Center (TV Series) Walter \\xa01974 McMillan & Wife (TV Series) Jerry \\xa01974 The Rookies (TV Series) Johnny Dane \\xa01973 Circle of Fear (TV Series) James Barlow \\xa01972 The F.B.I. (TV Series) Clifford Wade \\xa01972 Movin\\' On (TV Movie) Jeff \\xa01971 Dan August (TV Series) Lawrence Merrill III \\xa01967 Star Trek (TV Series) Makora \\xa02016 The Conjuring 2 (performer: \"Don\\'t Give Up On Us\") \\xa02013/I Filth (performer: \"Silver Lady\") \\xa02011 Johnny English Reborn (courtesy: \"Don\\'t Give Up On Us\") / (performer: \"Don\\'t Give Up On Us\") \\xa02010 Rabbit Hole (performer: \"Don\\'t Give Up On Us\") \\xa02007 The Hitcher (performer: \"Don\\'t Give Up on Us\") \\xa01977-1978 Top of the Pops (TV Series) (performer - 17 episodes) - Episode dated 22 June 1978 (1978) ... (performer: \"It Sure Brings Out the Love in Your Eyes\") - Episode dated 8 June 1978 (1978) ... (performer: \"It Sure Brings Out the Love in Your Eyes\")'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Splitting\n",
    "\n",
    "You might have noticed that the dataset is not split into subsets (it contains only the `train` subset). To maintain the good practice of working with ML, we should have three datasets: `train`, `validation`, and `test`. The code below splits our dataset into those three subsets. We set the size of both the `validation` and `test` sets as 10,000 and keep the rest in the `train` subset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:23.348770627Z",
     "start_time": "2026-02-11T12:18:23.243532753Z"
    }
   },
   "source": [
    "dataset = dataset['train'].train_test_split(test_size=10_000)\n",
    "valid_dataset = dataset['test']\n",
    "dataset = dataset['train'].train_test_split(test_size=10_000)\n",
    "dataset['validation'] = valid_dataset\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 53346\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cleaning\n",
    "\n",
    "Let's write the function to clean the text. It can be similar to the one from the previous lab (Lab1) but make sure that it makes sense for this dataset and task.\n",
    "\n",
    "More specifically, think about lower-casing, punctuation, stop-words and lemmatization/stemming and the impact it might have on the dataset. Also reflect on the fact that with word embeddings we want to uncover semantic relationships between words, whereas with bag-of-words we were trying to capture different morphological variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise 1: Clean function\n",
    "Fill in the following function to clean the dataset. Implement at least 6 different steps."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:23.398007699Z",
     "start_time": "2026-02-11T12:18:23.357369795Z"
    }
   },
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Cleans the text\n",
    "    Args:\n",
    "        text: a string that will be cleaned\n",
    "\n",
    "    Returns: the cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty text\n",
    "    if text == '':\n",
    "        return text\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    text = re.sub(r'(?<=\\d),(?=\\d)', '', text) # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub(r'([.,!?;:\"])', r' \\1 ', text) # space out the punctuation (i.e. \"hello, world.\" -> \"hello , world .\")\n",
    "\n",
    "    text = text.lower() # Extra step: making everything lowercase\n",
    "    text = re.sub(r'http\\S+', '', text) # Extra step: removes urls starting with https\n",
    "    text = re.sub(r'#', '', text) # Extra step: removes hashtags\n",
    "\n",
    "    # eliminate stop words\n",
    "    text = re.sub(r'\\b(a|an|the|and|or|but)\\b', '', text)\n",
    "\n",
    "    # substitute - with white spaces\n",
    "    text = re.sub(r'-', ' ', text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces at the end of the cleaning process, as some of the previous steps can introduce double spacing\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "sentence = 'Which American-born Sinclair won the Nobel Prize for Literature in 1930?'\n",
    "print('Testing the clean function:')\n",
    "print('Original:', sentence)\n",
    "print('Cleaned:', clean(sentence))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the clean function:\n",
      "Original: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
      "Cleaned: which american born sinclair won nobel prize for literature in 1930 \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function will apply the function you just wrote to the whole dataset. More specifically, it takes the `query` and `answer` fields, applies the `clean` function and saves the processed sentences back to the `query` and `answer` fields. This will override the original fields. If you want to have access to them, you can make a copy in separate fields before cleaning. As in the last lab, we will use the `map()` method of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:49.385843646Z",
     "start_time": "2026-02-11T12:18:23.400750122Z"
    }
   },
   "source": [
    "def clean_example(example):\n",
    "    \"\"\"\n",
    "    Applies the clean() function to the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example with cleaned 'query' and 'answer' columns\n",
    "\n",
    "    \"\"\"\n",
    "    example['original_query'] = example['query']\n",
    "    example['original_answer'] = example['answer']\n",
    "\n",
    "    example['query'] = clean(example['query'])\n",
    "    example['answer'] = clean(example['answer'])\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(clean_example, desc=\"Cleaning queries and answers\")\n",
    "print(dataset)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning queries and answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53346/53346 [00:19<00:00, 2780.92 examples/s]\n",
      "Cleaning queries and answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:03<00:00, 2914.38 examples/s]\n",
      "Cleaning queries and answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:03<00:00, 2997.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'answer', 'original_query', 'original_answer'],\n",
      "        num_rows: 53346\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'answer', 'original_query', 'original_answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'answer', 'original_query', 'original_answer'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine some examples from the dataset and make sure that we got the results we wanted. At this step, it might be necessary to revisit some pre-processing steps if you are not happy with the results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:49.473538983Z",
     "start_time": "2026-02-11T12:18:49.409156457Z"
    }
   },
   "source": [
    "for i in range(5):\n",
    "    print('example', i)\n",
    "    print('query', dataset['train'][i]['query'])\n",
    "    print('answer', dataset['train'][i]['answer'])\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0\n",
      "query  fictional character norville rogers is better known by nickname \n",
      "answer real names of 23 fictional characters cnn com real names of 23 fictional characters by jason english mental floss mr clean got first name during 1962 promotion story highlights many famous fictional characters are known only as their nicknames capn crunchs full name is captain horatio magellan crunch wizard of oz oscar zoroaster phadrig isaac norman henkel emmannuel ambroise diggs on entourage turtles real name is salvatore assante mental floss you know characters you might not know their full names store these away for future trivia nights 1 did you know comic book guy on simpsons has name its jeff albertson that wasnt decision of creator matt groening i was out of room when writers named him he told mtv in 2007 in my mind louis lane was his name he was obsessed tormented by lois lane 2 barbies full name is barbara millicent roberts kens last name is carson 3 capn crunchs full name is captain horatio magellan crunch his ship is s s guppy 4 in peanuts comic strip peppermint pattys real name is patricia reichardt 5 snuffleupagus has first name aloysius want more snuffleupagus trivia in sesame street scene that never aired snuffys parents announced they were separating in testing children were too devastated by news so idea was scrapped mental floss 9 muppets kicked off sesame street 6 wizard of oz rolls off tongue lot easier than man behind curtains full name oscar zoroaster phadrig isaac norman henkel emmannuel ambroise diggs from frank baums dorothy wizard in oz it was dreadfully long name to weigh down poor innocent child one of hardest lessons i ever learned was to remember my own name when i grew up i just called myself o z because other initials were p i n h e d that spelled pinhead which was reflection on my intelligence 7 mr clean has seldom used first name veritably name came from give mr clean first name promotion in 1962 8 in deleted scene in 2006 curious george movie man with yellow hats full name was revealed as ted shackleford since scene was deleted perhaps last name doesnt count 9 real name of monopoly mascot rich uncle pennybags is milburn pennybags 10 policeman in monopoly has name too you can thank officer edgar mallory next time he sends you to jail 11 on night court nostradamus shannon was better known as bull 12 on entourage turtles real name is salvatore assante 13 sesame streets resident game show host guy smiley was using pseudonym all these years he was born bernie liederkrantz 14 michelin mans name is bibendum 15 on gilligans island jonas grumby was simply called skipper 16 professor was roy hinkley stories behind graduation traditions 17 unkempt shaggy of scooby doo fame has rather proper real name norville rogers 18 pillsbury doughboys name is poppin fresh he has wife poppie fresh two kids popper bun bun 19 patient in classic game operation is cavity sam 20 true identity of lone ranger was john reid 21 macgyvers first name angus 22 23 ok these last two arent fictional just in case it comes up bono was born paul david hewson edges name is david howell evans for more mental_floss articles visit mentalfloss com entire contents of this article copyright mental floss llc all rights reserved share this on \n",
      "\n",
      "example 1\n",
      "query titania oberon are moons of which planet \n",
      "answer  moon oberon history structure orbit surface evolution moon oberon oberon based on nasa images william herschel national portrait gallery london oberon is outermost one among 27 moons of uranus william herschel discovered it in year 1787 it is second largest as well as second most massive moon of uranus he discovered oberon titania on same day in entire solar system it is ninth most massive moon much like all other moons of uranus oberon was also named in 1852 after character in shakespeares play midsummer nights dream though initially named as uranus ii according to order in which it was discovered it was later renamed as uranus iv according to its distance from planet physical structure of oberon oberon is assumed to have formed from diffuse material around uranus after planet was formed structure of oberon is that of rocky core covered by mantle made of ice however moon consists of equal amounts of rock ice it is assumed that layer of liquid water may be present between core mantle surface of oberon is slightly dark red in color is covered by number of craters that may have been created by impact of asteroids comets some of craters are almost 210 km in diameter surface of oberon also has many deep elongated depressions called chasmata oberon uranus nasa these are assumed to have been formed when interior of moon was still expanding during early years of its evolution it is believed that it is remained stable from days its formation radius of oberon is about 763 km has surface area of 7 2 million km density of oberon is about 1 63 gcm3 has surface gravity of about 0 348 ms2 temperature in moon varies between 70 80 degrees kelvin oberons orbit oberons orbit is at distance of 584000 km from uranus orbital period of moon is 13 5 days this coincides with its rotational period as result one face of moon is tidally locked with uranus that is it always faces planet oberon spends major part of its orbit outside magnetosphere of uranus this is reason why its surface is struck by solar wind directly for those satellites that orbit inside magnetosphere trailing hemispheres of these moons are continuously struck by magnetospheric plasma which leads to darkening of these hemispheres unlike that of oberon uranus orbits sun almost on its side moons of uranus orbit planet on its equatorial plane this causes satellites to experience extreme seasonal cycles north south poles of oberon have 42 years of continuous darkness next 42 years of continuous sunlight during each solstice sun rises close to zenith of one of poles during summer solstice northern hemisphere is left totally darkened oberons internal structure oberons density is indicative of fact that it contains equal proportions of water ice as well as non ice dense component non ice material is believed to be made up of carbonaceous material organic compounds rock spectroscopic findings have corroborated presence of water ice on surface of oberon internal structure of oberon nasa images whereas other uranian moons exhibit more water ice presence in their leading hemispheres oberon exhibits stronger signatures in its trailing hemisphere this discrepancy is seen as result of formation of soil due to impacts which is more predominant on its leading hemisphere impacts from meteorites generally knock off ice from surface leave behind material that is non ice dark this darkening of this material is assumed to have been caused by radiation processing of compounds of organic nature radius of core of oberon is about 480 km pressure at center of core is about 5 kbar exact composition of icy mantle is unclear in case mantle contains sufficient quantity of antifreeze \n",
      "\n",
      "example 2\n",
      "query what popular sandwich consists of hamburger patty sautƒÉ≈°ed onions swiss cheese on rye bread which is then grilled \n",
      "answer hamburger wikipedia photos videos hamburger next go to results 51 100 wikipedia article jump to navigation search this article is about sandwich for meat served as part of such sandwich see patty for other uses see hamburger disambiguation hamburger hamburger on roll with french fries course media hamburger hamburger cheeseburger when served with slice of cheese is sandwich consisting of one more cooked patties of ground meat usually beef placed inside sliced bread roll bun hamburgers may be cooked in variety of ways including pan frying barbecuing flame broiling hamburgers are often served with cheese lettuce tomato bacon onion pickles condiments such as mustard mayonnaise ketchup relish chiles 1 term burger can also be applied to meat patty on its own especially in uk where term patty is rarely used term may be prefixed with type of meat meat substitute used as in turkey burger bison burger veggie burger hamburgers are sold at fast food restaurants diners specialty high end restaurants where burgers may sell for several times cost of fast food burger there are many international regional variations of hamburger contents etymology terminology hamburger is named after hamburg germany term hamburger originally derives from hamburg 2 germany s second largest city in german burg means castle fortified settlement fortified refuge is widespread component of place names first element of name is perhaps from old high german hamma referring to bend in river middle high german hamme referring to enclosed area of pastureland 3 hamburger in german is demonym of hamburg similar to frankfurter wiener names for other meat based foods demonyms of cities of frankfurt vienna wien respectively term burger back formation is associated with many different types of sandwiches similar to ground meat hamburger made of different meats such as buffalo in buffalo burger venison kangaroo turkey elk lamb fish like salmon in salmon burger even with meatless sandwiches as is case of veggie burger 4 history main articles history of hamburger history of hamburger in united states there have been many claims about origin of hamburger there is reference to hamburg steak as early as 1884 in boston journal oed under steak on july 5 1896 chicago daily tribune made highly specific claim regarding hamburger sandwich in article about sandwich car distinguished favorite only five cents is hamburger steak sandwich meat for which is kept ready in small patties cooked while you wait on gasoline range 5 according to congresswoman rosa delauro hamburger ground meat patty between two slices of bread was first created in america in 1900 by louis lassen danish immigrant owner of louis lunch in new haven connecticut 6 there have been rival claims by charlie nagreen frank charles menches oscar weber bilby fletcher davis 7 8 white castle traces origin of hamburger to hamburg germany with its invention by otto kuase 9 however it gained national recognition at 1904 st louis worlds fair when new york tribune referred to hamburger as innovation of food vendor on pike 8 no conclusive argument has ever ended dispute over invention article from abc news sums up one problem is that there is little written history another issue is that spread of burger happened largely at worlds fair from tiny vendors that came went in instant it is entirely possible that more than one person came up with idea at same time in different parts of country 10 claims of invention louis lassen louis lassen of louis lunch small lunch wagon in new haven conn\n",
      "\n",
      "example 3\n",
      "query whales can be divided into two types toothed what \n",
      "answer feeding whales feeding reproducing feeding whales can be divided into two types by way they feedbaleen whales mysticetes toothed whales odontecetes baleen whales are batch feedersthey use their plates of baleen to filter huge numbers of tiny prey out of water they can be divided into three groups according to how they filter their food rorquals such as brydes whale are gulpers they take in huge amounts of water with their prey their pleated throats billowing out to accommodate it then they force water out with their tongues straining food through mesh of baleen right whales skim feed they cruise through water with their enormous arched mouths open continuously filtering out prey gray whales are silt sifters they pump water sediment from seafloor through one side of their mouths across baleen out other toothed whales tend to eat individual animals such as squid fish in some cases other marine mammals these whales locate their prey by using echolocationlike human sonar they either seize their prey with their teeth suck them directly into their mouths swallow them whole \n",
      "\n",
      "example 4\n",
      "query garnet comes in many colours what is commonest\n",
      "answer  colors varieties of garnet colors varieties of garnet colors varieties of garnet published may 2010 by cara williams f g garnets are not just family they are more like clan they have much in common same crystal form cubic same chemical outline i say outline because in many ways chemistry varies between different types of garnets so while we call them family their dna varies enough to call them clan gemologists recognize six garnet families within garnet clan five are well known pyrope almandine spessartite grossular andradite sixth uvarovite is hardly ever seen heard from if you didnt catch name of your favorite hold on we will get to it in part 2 of this report besides these six gem garnets mineralogists know about more than 30 others some just theoretical yes there is lot we didnt know about that january birthstone read more about garnet in our gemopedia garnets adhere to three part formula there is silicate component as well as two other elements ratio of these components does not vary for more detail please look at chart below six garnet families tend to hang out in two groups aluminum group calcium group they occasionally mix intermarry not much aluminum calcium silicate do not cause color other elements do those wild genes aluminum garnets are called pyralspites word formed from individual names pyrope almandine spessartite calcium garnets are ugrandites uvarovite grossular andradite now imagine garnets as very old established clan while individuals might have certain last name they are not just jones smith they are combination of whatever their ancestors were so we never have pure pyrope pure almandine pyralspites pure pyrope would be colorless we all know there are no colorless garnets pyropes are always red getting their color from some almandine ancestry sometimes from bit of uvarovite bits of uvarovite genes molecules pop up in many garnets dna almandine is classic red garnet possibly most common type it has iron in its formula which creates color in this case strong red this is why it does not take much almandine in garnet to make it red spessartite is third pyralspite it has manganese in its formula this manganese creates yellowish orange color because many of spessartites ancestors married almandines many are reddish orange due to some iron lighter orange spessartites similar in color to orange soft drinks are almost pure spessartites ugrandites other group of gem garnets ugrandites is not as well known as plentiful as pyralspites uvarovite rich emerald green has chromium in its formula which stunts crystal growth this garnet is rarely seen as anything other than drusy lots of tiny crystals all close together however it leaves its mark on other garnets more than you might think grossular garnet has aluminum calcium in its formula neither of which creates color however grossulars can range widely in their shades are often close to pure they get their color from traces of other garnets mixed in andradite is last garnet is different from others in several ways it has iron in different state position within structure than almandine therefore it does not create red instead andradites can be golden brown even other colors if mixed garnet varieties part 2 i suspect many of garnet names we covered in part 1 may be new to some that is because various garnets are usually called by trade names that were created as different finds were discovered because no garnet is pure many are composed of combinations of three more garnets it is too confusing to stick to gemological names so we have always named these varieties on their appearance gemological properties origin regardless of what act\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extracting answers\n",
    "\n",
    "Because the answers in our dataset are not unique, we will extract them and create a separate dataset containing only the unique answers. We will do this for each split separately."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:49.516445511Z",
     "start_time": "2026-02-11T12:18:49.479168457Z"
    }
   },
   "source": [
    "def get_answers(subset):\n",
    "    \"\"\"\n",
    "    Extracts unique answers from the subset of the dataset and builds a dictionary with answers as keys and ids as values.\n",
    "    Args:\n",
    "        subset: a subset of the dataset\n",
    "\n",
    "    Returns: a dictionary mapping answers to their ids\n",
    "    \"\"\"\n",
    "    answer_to_id = {}\n",
    "    answers = list(set(subset['answer']))\n",
    "    for i, answer in enumerate(answers):\n",
    "        answer_to_id[answer] = i\n",
    "    return answer_to_id"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We apply this function separately to each subset and create the answers dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:51.528685304Z",
     "start_time": "2026-02-11T12:18:49.521812061Z"
    }
   },
   "source": [
    "train_answer_to_id = get_answers(dataset['train'])\n",
    "valid_answer_to_id = get_answers(dataset['validation'])\n",
    "test_answer_to_id = get_answers(dataset['test'])\n",
    "\n",
    "answers_dataset = DatasetDict({\n",
    "    'train': datasets.Dataset.from_dict({'id': range(len(train_answer_to_id)), 'answer': train_answer_to_id.keys()}),\n",
    "    'validation': datasets.Dataset.from_dict(\n",
    "        {'id': range(len(valid_answer_to_id)), 'answer': valid_answer_to_id.keys()}),\n",
    "    'test': datasets.Dataset.from_dict({'id': range(len(test_answer_to_id)), 'answer': test_answer_to_id.keys()})\n",
    "})\n",
    "print(answers_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'answer'],\n",
      "        num_rows: 47916\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'answer'],\n",
      "        num_rows: 9756\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'answer'],\n",
      "        num_rows: 9743\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The last thing we will have to do is to connect the answers in the original dataset to the ids of answers (in the answers dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise 2: Setting answer ids\n",
    "Fill in the following function to find and set the `answer_id` field with the id of the answer. The function accepts one of the `answer_to_id` dictionaries that you just created."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:51.589653154Z",
     "start_time": "2026-02-11T12:18:51.573785989Z"
    }
   },
   "source": [
    "def set_answer_id(example, answer_to_id):\n",
    "    \"\"\"\n",
    "    Sets the answer_id field in the example based on the answer_to_id dictionary\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        answer_to_id: a dictionary mapping answers to their ids\n",
    "\n",
    "    Returns: the updated example with the 'answer_id' field\n",
    "    \"\"\"\n",
    "    answer = example['answer']\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    example['answer_id'] = answer_to_id[answer]\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return example"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we apply the function to each split separately making sure to pass the correct `answer_to_id` dictionary. We also remove the `answer` columns from the original dataset, as now we can reference the correct answer through the `answer_id` field."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:55.405156825Z",
     "start_time": "2026-02-11T12:18:51.594268924Z"
    }
   },
   "source": [
    "dataset['train'] = dataset['train'].map(set_answer_id,\n",
    "                                        fn_kwargs={'answer_to_id': train_answer_to_id},\n",
    "                                        desc=\"Setting ids for answers (train)\")\n",
    "dataset['validation'] = dataset['validation'].map(set_answer_id,\n",
    "                                                  fn_kwargs={'answer_to_id': valid_answer_to_id},\n",
    "                                                  desc=\"Setting ids for answers (validation)\")\n",
    "dataset['test'] = dataset['test'].map(set_answer_id,\n",
    "                                      fn_kwargs={'answer_to_id': test_answer_to_id},\n",
    "                                      desc=\"Setting ids for answers (test\")\n",
    "\n",
    "dataset = dataset.remove_columns('answer')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting ids for answers (train): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53346/53346 [00:02<00:00, 21700.66 examples/s]\n",
      "Setting ids for answers (validation): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 20602.85 examples/s]\n",
      "Setting ids for answers (test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 23633.29 examples/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tokenizing\n",
    "\n",
    "<a name='e3'></a>\n",
    "#### Exercise 3: Tokenizing\n",
    "As always, we will need to tokenize the dataset in order to create bat-of-words and TF-IDF representations in the next sections. You can use the function from the previous lab or use a library such as [Natural Language Toolkit (NLTK) library]([https://www.nltk.org/]) (https://www.nltk.org/). Complete the following function to split the text into tokens.\n",
    "\n",
    "Contrary to the previous lab, we will not include the special tokens (unknown, beginning, and end of the sequence)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:18:55.468170317Z",
     "start_time": "2026-02-11T12:18:55.450322030Z"
    }
   },
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes the text that is assumed to be cleaned first with the clean() function. The tokenized sequence should start with the `bos_token` token and end with the 'eos_token'.\n",
    "    Args:\n",
    "        text: a cleaned text\n",
    "\n",
    "    Returns: tokenized text as a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = None  # list of tokens, your code should fill this variable\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # we use nltk library to tokenize the text\n",
    "    # tolenizer model downloaded under imports\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return tokens"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We apply your function to both the `query` field in the original dataset and `answer` field in the answers dataset. We save the tokenized queries in `query_tokens` field and answers in `answer_tokens` field."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:19:52.262770342Z",
     "start_time": "2026-02-11T12:18:55.473421369Z"
    }
   },
   "source": [
    "def tokenize_example(example, src_column, tgt_column):\n",
    "    \"\"\"\n",
    "    Applies the tokenize() function to the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example containing 'query_tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    query = example[src_column]\n",
    "    example[tgt_column] = tokenize(query)\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_example,\n",
    "                      fn_kwargs={'src_column': 'query', 'tgt_column': 'query_tokens'},\n",
    "                      desc=\"Tokenizing queries\")\n",
    "print('dataset')\n",
    "print(dataset)\n",
    "\n",
    "answers_dataset = answers_dataset.map(tokenize_example,\n",
    "                                      fn_kwargs={'src_column': 'answer', 'tgt_column': 'answer_tokens'},\n",
    "                                      desc=\"Tokenizing answers\")\n",
    "print('answers_dataset')\n",
    "print(answers_dataset)"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53346/53346 [00:03<00:00, 13445.49 examples/s]\n",
      "Tokenizing queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 13411.56 examples/s]\n",
      "Tokenizing queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 9927.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['query', 'original_query', 'original_answer', 'answer_id', 'query_tokens'],\n",
      "        num_rows: 53346\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['query', 'original_query', 'original_answer', 'answer_id', 'query_tokens'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['query', 'original_query', 'original_answer', 'answer_id', 'query_tokens'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47916/47916 [00:36<00:00, 1316.02 examples/s]\n",
      "Tokenizing answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9756/9756 [00:07<00:00, 1347.65 examples/s]\n",
      "Tokenizing answers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9743/9743 [00:07<00:00, 1320.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers_dataset\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'answer', 'answer_tokens'],\n",
      "        num_rows: 47916\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'answer', 'answer_tokens'],\n",
      "        num_rows: 9756\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'answer', 'answer_tokens'],\n",
      "        num_rows: 9743\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine some examples of tokenized queries and answers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:19:52.350002416Z",
     "start_time": "2026-02-11T12:19:52.305962832Z"
    }
   },
   "source": [
    "for i in range(5):\n",
    "    print('example:', i)\n",
    "    print('query:', dataset['train'][i]['query'])\n",
    "    print('query_tokens:', dataset['train'][i]['query_tokens'])\n",
    "    answer_id = dataset['train'][i]['answer_id']\n",
    "    print('answer_id:', answer_id)\n",
    "    print('answer:', answers_dataset['train'][answer_id]['answer'])\n",
    "    print('answer_tokens:', answers_dataset['train'][answer_id]['answer_tokens'])\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: 0\n",
      "query:  fictional character norville rogers is better known by nickname \n",
      "query_tokens: ['fictional', 'character', 'norville', 'rogers', 'is', 'better', 'known', 'by', 'nickname']\n",
      "answer_id: 8027\n",
      "answer: real names of 23 fictional characters cnn com real names of 23 fictional characters by jason english mental floss mr clean got first name during 1962 promotion story highlights many famous fictional characters are known only as their nicknames capn crunchs full name is captain horatio magellan crunch wizard of oz oscar zoroaster phadrig isaac norman henkel emmannuel ambroise diggs on entourage turtles real name is salvatore assante mental floss you know characters you might not know their full names store these away for future trivia nights 1 did you know comic book guy on simpsons has name its jeff albertson that wasnt decision of creator matt groening i was out of room when writers named him he told mtv in 2007 in my mind louis lane was his name he was obsessed tormented by lois lane 2 barbies full name is barbara millicent roberts kens last name is carson 3 capn crunchs full name is captain horatio magellan crunch his ship is s s guppy 4 in peanuts comic strip peppermint pattys real name is patricia reichardt 5 snuffleupagus has first name aloysius want more snuffleupagus trivia in sesame street scene that never aired snuffys parents announced they were separating in testing children were too devastated by news so idea was scrapped mental floss 9 muppets kicked off sesame street 6 wizard of oz rolls off tongue lot easier than man behind curtains full name oscar zoroaster phadrig isaac norman henkel emmannuel ambroise diggs from frank baums dorothy wizard in oz it was dreadfully long name to weigh down poor innocent child one of hardest lessons i ever learned was to remember my own name when i grew up i just called myself o z because other initials were p i n h e d that spelled pinhead which was reflection on my intelligence 7 mr clean has seldom used first name veritably name came from give mr clean first name promotion in 1962 8 in deleted scene in 2006 curious george movie man with yellow hats full name was revealed as ted shackleford since scene was deleted perhaps last name doesnt count 9 real name of monopoly mascot rich uncle pennybags is milburn pennybags 10 policeman in monopoly has name too you can thank officer edgar mallory next time he sends you to jail 11 on night court nostradamus shannon was better known as bull 12 on entourage turtles real name is salvatore assante 13 sesame streets resident game show host guy smiley was using pseudonym all these years he was born bernie liederkrantz 14 michelin mans name is bibendum 15 on gilligans island jonas grumby was simply called skipper 16 professor was roy hinkley stories behind graduation traditions 17 unkempt shaggy of scooby doo fame has rather proper real name norville rogers 18 pillsbury doughboys name is poppin fresh he has wife poppie fresh two kids popper bun bun 19 patient in classic game operation is cavity sam 20 true identity of lone ranger was john reid 21 macgyvers first name angus 22 23 ok these last two arent fictional just in case it comes up bono was born paul david hewson edges name is david howell evans for more mental_floss articles visit mentalfloss com entire contents of this article copyright mental floss llc all rights reserved share this on \n",
      "answer_tokens: ['real', 'names', 'of', '23', 'fictional', 'characters', 'cnn', 'com', 'real', 'names', 'of', '23', 'fictional', 'characters', 'by', 'jason', 'english', 'mental', 'floss', 'mr', 'clean', 'got', 'first', 'name', 'during', '1962', 'promotion', 'story', 'highlights', 'many', 'famous', 'fictional', 'characters', 'are', 'known', 'only', 'as', 'their', 'nicknames', 'capn', 'crunchs', 'full', 'name', 'is', 'captain', 'horatio', 'magellan', 'crunch', 'wizard', 'of', 'oz', 'oscar', 'zoroaster', 'phadrig', 'isaac', 'norman', 'henkel', 'emmannuel', 'ambroise', 'diggs', 'on', 'entourage', 'turtles', 'real', 'name', 'is', 'salvatore', 'assante', 'mental', 'floss', 'you', 'know', 'characters', 'you', 'might', 'not', 'know', 'their', 'full', 'names', 'store', 'these', 'away', 'for', 'future', 'trivia', 'nights', '1', 'did', 'you', 'know', 'comic', 'book', 'guy', 'on', 'simpsons', 'has', 'name', 'its', 'jeff', 'albertson', 'that', 'wasnt', 'decision', 'of', 'creator', 'matt', 'groening', 'i', 'was', 'out', 'of', 'room', 'when', 'writers', 'named', 'him', 'he', 'told', 'mtv', 'in', '2007', 'in', 'my', 'mind', 'louis', 'lane', 'was', 'his', 'name', 'he', 'was', 'obsessed', 'tormented', 'by', 'lois', 'lane', '2', 'barbies', 'full', 'name', 'is', 'barbara', 'millicent', 'roberts', 'kens', 'last', 'name', 'is', 'carson', '3', 'capn', 'crunchs', 'full', 'name', 'is', 'captain', 'horatio', 'magellan', 'crunch', 'his', 'ship', 'is', 's', 's', 'guppy', '4', 'in', 'peanuts', 'comic', 'strip', 'peppermint', 'pattys', 'real', 'name', 'is', 'patricia', 'reichardt', '5', 'snuffleupagus', 'has', 'first', 'name', 'aloysius', 'want', 'more', 'snuffleupagus', 'trivia', 'in', 'sesame', 'street', 'scene', 'that', 'never', 'aired', 'snuffys', 'parents', 'announced', 'they', 'were', 'separating', 'in', 'testing', 'children', 'were', 'too', 'devastated', 'by', 'news', 'so', 'idea', 'was', 'scrapped', 'mental', 'floss', '9', 'muppets', 'kicked', 'off', 'sesame', 'street', '6', 'wizard', 'of', 'oz', 'rolls', 'off', 'tongue', 'lot', 'easier', 'than', 'man', 'behind', 'curtains', 'full', 'name', 'oscar', 'zoroaster', 'phadrig', 'isaac', 'norman', 'henkel', 'emmannuel', 'ambroise', 'diggs', 'from', 'frank', 'baums', 'dorothy', 'wizard', 'in', 'oz', 'it', 'was', 'dreadfully', 'long', 'name', 'to', 'weigh', 'down', 'poor', 'innocent', 'child', 'one', 'of', 'hardest', 'lessons', 'i', 'ever', 'learned', 'was', 'to', 'remember', 'my', 'own', 'name', 'when', 'i', 'grew', 'up', 'i', 'just', 'called', 'myself', 'o', 'z', 'because', 'other', 'initials', 'were', 'p', 'i', 'n', 'h', 'e', 'd', 'that', 'spelled', 'pinhead', 'which', 'was', 'reflection', 'on', 'my', 'intelligence', '7', 'mr', 'clean', 'has', 'seldom', 'used', 'first', 'name', 'veritably', 'name', 'came', 'from', 'give', 'mr', 'clean', 'first', 'name', 'promotion', 'in', '1962', '8', 'in', 'deleted', 'scene', 'in', '2006', 'curious', 'george', 'movie', 'man', 'with', 'yellow', 'hats', 'full', 'name', 'was', 'revealed', 'as', 'ted', 'shackleford', 'since', 'scene', 'was', 'deleted', 'perhaps', 'last', 'name', 'doesnt', 'count', '9', 'real', 'name', 'of', 'monopoly', 'mascot', 'rich', 'uncle', 'pennybags', 'is', 'milburn', 'pennybags', '10', 'policeman', 'in', 'monopoly', 'has', 'name', 'too', 'you', 'can', 'thank', 'officer', 'edgar', 'mallory', 'next', 'time', 'he', 'sends', 'you', 'to', 'jail', '11', 'on', 'night', 'court', 'nostradamus', 'shannon', 'was', 'better', 'known', 'as', 'bull', '12', 'on', 'entourage', 'turtles', 'real', 'name', 'is', 'salvatore', 'assante', '13', 'sesame', 'streets', 'resident', 'game', 'show', 'host', 'guy', 'smiley', 'was', 'using', 'pseudonym', 'all', 'these', 'years', 'he', 'was', 'born', 'bernie', 'liederkrantz', '14', 'michelin', 'mans', 'name', 'is', 'bibendum', '15', 'on', 'gilligans', 'island', 'jonas', 'grumby', 'was', 'simply', 'called', 'skipper', '16', 'professor', 'was', 'roy', 'hinkley', 'stories', 'behind', 'graduation', 'traditions', '17', 'unkempt', 'shaggy', 'of', 'scooby', 'doo', 'fame', 'has', 'rather', 'proper', 'real', 'name', 'norville', 'rogers', '18', 'pillsbury', 'doughboys', 'name', 'is', 'poppin', 'fresh', 'he', 'has', 'wife', 'poppie', 'fresh', 'two', 'kids', 'popper', 'bun', 'bun', '19', 'patient', 'in', 'classic', 'game', 'operation', 'is', 'cavity', 'sam', '20', 'true', 'identity', 'of', 'lone', 'ranger', 'was', 'john', 'reid', '21', 'macgyvers', 'first', 'name', 'angus', '22', '23', 'ok', 'these', 'last', 'two', 'arent', 'fictional', 'just', 'in', 'case', 'it', 'comes', 'up', 'bono', 'was', 'born', 'paul', 'david', 'hewson', 'edges', 'name', 'is', 'david', 'howell', 'evans', 'for', 'more', 'mental_floss', 'articles', 'visit', 'mentalfloss', 'com', 'entire', 'contents', 'of', 'this', 'article', 'copyright', 'mental', 'floss', 'llc', 'all', 'rights', 'reserved', 'share', 'this', 'on']\n",
      "\n",
      "example: 1\n",
      "query: titania oberon are moons of which planet \n",
      "query_tokens: ['titania', 'oberon', 'are', 'moons', 'of', 'which', 'planet']\n",
      "answer_id: 12429\n",
      "answer:  moon oberon history structure orbit surface evolution moon oberon oberon based on nasa images william herschel national portrait gallery london oberon is outermost one among 27 moons of uranus william herschel discovered it in year 1787 it is second largest as well as second most massive moon of uranus he discovered oberon titania on same day in entire solar system it is ninth most massive moon much like all other moons of uranus oberon was also named in 1852 after character in shakespeares play midsummer nights dream though initially named as uranus ii according to order in which it was discovered it was later renamed as uranus iv according to its distance from planet physical structure of oberon oberon is assumed to have formed from diffuse material around uranus after planet was formed structure of oberon is that of rocky core covered by mantle made of ice however moon consists of equal amounts of rock ice it is assumed that layer of liquid water may be present between core mantle surface of oberon is slightly dark red in color is covered by number of craters that may have been created by impact of asteroids comets some of craters are almost 210 km in diameter surface of oberon also has many deep elongated depressions called chasmata oberon uranus nasa these are assumed to have been formed when interior of moon was still expanding during early years of its evolution it is believed that it is remained stable from days its formation radius of oberon is about 763 km has surface area of 7 2 million km density of oberon is about 1 63 gcm3 has surface gravity of about 0 348 ms2 temperature in moon varies between 70 80 degrees kelvin oberons orbit oberons orbit is at distance of 584000 km from uranus orbital period of moon is 13 5 days this coincides with its rotational period as result one face of moon is tidally locked with uranus that is it always faces planet oberon spends major part of its orbit outside magnetosphere of uranus this is reason why its surface is struck by solar wind directly for those satellites that orbit inside magnetosphere trailing hemispheres of these moons are continuously struck by magnetospheric plasma which leads to darkening of these hemispheres unlike that of oberon uranus orbits sun almost on its side moons of uranus orbit planet on its equatorial plane this causes satellites to experience extreme seasonal cycles north south poles of oberon have 42 years of continuous darkness next 42 years of continuous sunlight during each solstice sun rises close to zenith of one of poles during summer solstice northern hemisphere is left totally darkened oberons internal structure oberons density is indicative of fact that it contains equal proportions of water ice as well as non ice dense component non ice material is believed to be made up of carbonaceous material organic compounds rock spectroscopic findings have corroborated presence of water ice on surface of oberon internal structure of oberon nasa images whereas other uranian moons exhibit more water ice presence in their leading hemispheres oberon exhibits stronger signatures in its trailing hemisphere this discrepancy is seen as result of formation of soil due to impacts which is more predominant on its leading hemisphere impacts from meteorites generally knock off ice from surface leave behind material that is non ice dark this darkening of this material is assumed to have been caused by radiation processing of compounds of organic nature radius of core of oberon is about 480 km pressure at center of core is about 5 kbar exact composition of icy mantle is unclear in case mantle contains sufficient quantity of antifreeze \n",
      "answer_tokens: ['moon', 'oberon', 'history', 'structure', 'orbit', 'surface', 'evolution', 'moon', 'oberon', 'oberon', 'based', 'on', 'nasa', 'images', 'william', 'herschel', 'national', 'portrait', 'gallery', 'london', 'oberon', 'is', 'outermost', 'one', 'among', '27', 'moons', 'of', 'uranus', 'william', 'herschel', 'discovered', 'it', 'in', 'year', '1787', 'it', 'is', 'second', 'largest', 'as', 'well', 'as', 'second', 'most', 'massive', 'moon', 'of', 'uranus', 'he', 'discovered', 'oberon', 'titania', 'on', 'same', 'day', 'in', 'entire', 'solar', 'system', 'it', 'is', 'ninth', 'most', 'massive', 'moon', 'much', 'like', 'all', 'other', 'moons', 'of', 'uranus', 'oberon', 'was', 'also', 'named', 'in', '1852', 'after', 'character', 'in', 'shakespeares', 'play', 'midsummer', 'nights', 'dream', 'though', 'initially', 'named', 'as', 'uranus', 'ii', 'according', 'to', 'order', 'in', 'which', 'it', 'was', 'discovered', 'it', 'was', 'later', 'renamed', 'as', 'uranus', 'iv', 'according', 'to', 'its', 'distance', 'from', 'planet', 'physical', 'structure', 'of', 'oberon', 'oberon', 'is', 'assumed', 'to', 'have', 'formed', 'from', 'diffuse', 'material', 'around', 'uranus', 'after', 'planet', 'was', 'formed', 'structure', 'of', 'oberon', 'is', 'that', 'of', 'rocky', 'core', 'covered', 'by', 'mantle', 'made', 'of', 'ice', 'however', 'moon', 'consists', 'of', 'equal', 'amounts', 'of', 'rock', 'ice', 'it', 'is', 'assumed', 'that', 'layer', 'of', 'liquid', 'water', 'may', 'be', 'present', 'between', 'core', 'mantle', 'surface', 'of', 'oberon', 'is', 'slightly', 'dark', 'red', 'in', 'color', 'is', 'covered', 'by', 'number', 'of', 'craters', 'that', 'may', 'have', 'been', 'created', 'by', 'impact', 'of', 'asteroids', 'comets', 'some', 'of', 'craters', 'are', 'almost', '210', 'km', 'in', 'diameter', 'surface', 'of', 'oberon', 'also', 'has', 'many', 'deep', 'elongated', 'depressions', 'called', 'chasmata', 'oberon', 'uranus', 'nasa', 'these', 'are', 'assumed', 'to', 'have', 'been', 'formed', 'when', 'interior', 'of', 'moon', 'was', 'still', 'expanding', 'during', 'early', 'years', 'of', 'its', 'evolution', 'it', 'is', 'believed', 'that', 'it', 'is', 'remained', 'stable', 'from', 'days', 'its', 'formation', 'radius', 'of', 'oberon', 'is', 'about', '763', 'km', 'has', 'surface', 'area', 'of', '7', '2', 'million', 'km', 'density', 'of', 'oberon', 'is', 'about', '1', '63', 'gcm3', 'has', 'surface', 'gravity', 'of', 'about', '0', '348', 'ms2', 'temperature', 'in', 'moon', 'varies', 'between', '70', '80', 'degrees', 'kelvin', 'oberons', 'orbit', 'oberons', 'orbit', 'is', 'at', 'distance', 'of', '584000', 'km', 'from', 'uranus', 'orbital', 'period', 'of', 'moon', 'is', '13', '5', 'days', 'this', 'coincides', 'with', 'its', 'rotational', 'period', 'as', 'result', 'one', 'face', 'of', 'moon', 'is', 'tidally', 'locked', 'with', 'uranus', 'that', 'is', 'it', 'always', 'faces', 'planet', 'oberon', 'spends', 'major', 'part', 'of', 'its', 'orbit', 'outside', 'magnetosphere', 'of', 'uranus', 'this', 'is', 'reason', 'why', 'its', 'surface', 'is', 'struck', 'by', 'solar', 'wind', 'directly', 'for', 'those', 'satellites', 'that', 'orbit', 'inside', 'magnetosphere', 'trailing', 'hemispheres', 'of', 'these', 'moons', 'are', 'continuously', 'struck', 'by', 'magnetospheric', 'plasma', 'which', 'leads', 'to', 'darkening', 'of', 'these', 'hemispheres', 'unlike', 'that', 'of', 'oberon', 'uranus', 'orbits', 'sun', 'almost', 'on', 'its', 'side', 'moons', 'of', 'uranus', 'orbit', 'planet', 'on', 'its', 'equatorial', 'plane', 'this', 'causes', 'satellites', 'to', 'experience', 'extreme', 'seasonal', 'cycles', 'north', 'south', 'poles', 'of', 'oberon', 'have', '42', 'years', 'of', 'continuous', 'darkness', 'next', '42', 'years', 'of', 'continuous', 'sunlight', 'during', 'each', 'solstice', 'sun', 'rises', 'close', 'to', 'zenith', 'of', 'one', 'of', 'poles', 'during', 'summer', 'solstice', 'northern', 'hemisphere', 'is', 'left', 'totally', 'darkened', 'oberons', 'internal', 'structure', 'oberons', 'density', 'is', 'indicative', 'of', 'fact', 'that', 'it', 'contains', 'equal', 'proportions', 'of', 'water', 'ice', 'as', 'well', 'as', 'non', 'ice', 'dense', 'component', 'non', 'ice', 'material', 'is', 'believed', 'to', 'be', 'made', 'up', 'of', 'carbonaceous', 'material', 'organic', 'compounds', 'rock', 'spectroscopic', 'findings', 'have', 'corroborated', 'presence', 'of', 'water', 'ice', 'on', 'surface', 'of', 'oberon', 'internal', 'structure', 'of', 'oberon', 'nasa', 'images', 'whereas', 'other', 'uranian', 'moons', 'exhibit', 'more', 'water', 'ice', 'presence', 'in', 'their', 'leading', 'hemispheres', 'oberon', 'exhibits', 'stronger', 'signatures', 'in', 'its', 'trailing', 'hemisphere', 'this', 'discrepancy', 'is', 'seen', 'as', 'result', 'of', 'formation', 'of', 'soil', 'due', 'to', 'impacts', 'which', 'is', 'more', 'predominant', 'on', 'its', 'leading', 'hemisphere', 'impacts', 'from', 'meteorites', 'generally', 'knock', 'off', 'ice', 'from', 'surface', 'leave', 'behind', 'material', 'that', 'is', 'non', 'ice', 'dark', 'this', 'darkening', 'of', 'this', 'material', 'is', 'assumed', 'to', 'have', 'been', 'caused', 'by', 'radiation', 'processing', 'of', 'compounds', 'of', 'organic', 'nature', 'radius', 'of', 'core', 'of', 'oberon', 'is', 'about', '480', 'km', 'pressure', 'at', 'center', 'of', 'core', 'is', 'about', '5', 'kbar', 'exact', 'composition', 'of', 'icy', 'mantle', 'is', 'unclear', 'in', 'case', 'mantle', 'contains', 'sufficient', 'quantity', 'of', 'antifreeze']\n",
      "\n",
      "example: 2\n",
      "query: what popular sandwich consists of hamburger patty sautƒÉ≈°ed onions swiss cheese on rye bread which is then grilled \n",
      "query_tokens: ['what', 'popular', 'sandwich', 'consists', 'of', 'hamburger', 'patty', 'sautƒÉ≈°ed', 'onions', 'swiss', 'cheese', 'on', 'rye', 'bread', 'which', 'is', 'then', 'grilled']\n",
      "answer_id: 37153\n",
      "answer: hamburger wikipedia photos videos hamburger next go to results 51 100 wikipedia article jump to navigation search this article is about sandwich for meat served as part of such sandwich see patty for other uses see hamburger disambiguation hamburger hamburger on roll with french fries course media hamburger hamburger cheeseburger when served with slice of cheese is sandwich consisting of one more cooked patties of ground meat usually beef placed inside sliced bread roll bun hamburgers may be cooked in variety of ways including pan frying barbecuing flame broiling hamburgers are often served with cheese lettuce tomato bacon onion pickles condiments such as mustard mayonnaise ketchup relish chiles 1 term burger can also be applied to meat patty on its own especially in uk where term patty is rarely used term may be prefixed with type of meat meat substitute used as in turkey burger bison burger veggie burger hamburgers are sold at fast food restaurants diners specialty high end restaurants where burgers may sell for several times cost of fast food burger there are many international regional variations of hamburger contents etymology terminology hamburger is named after hamburg germany term hamburger originally derives from hamburg 2 germany s second largest city in german burg means castle fortified settlement fortified refuge is widespread component of place names first element of name is perhaps from old high german hamma referring to bend in river middle high german hamme referring to enclosed area of pastureland 3 hamburger in german is demonym of hamburg similar to frankfurter wiener names for other meat based foods demonyms of cities of frankfurt vienna wien respectively term burger back formation is associated with many different types of sandwiches similar to ground meat hamburger made of different meats such as buffalo in buffalo burger venison kangaroo turkey elk lamb fish like salmon in salmon burger even with meatless sandwiches as is case of veggie burger 4 history main articles history of hamburger history of hamburger in united states there have been many claims about origin of hamburger there is reference to hamburg steak as early as 1884 in boston journal oed under steak on july 5 1896 chicago daily tribune made highly specific claim regarding hamburger sandwich in article about sandwich car distinguished favorite only five cents is hamburger steak sandwich meat for which is kept ready in small patties cooked while you wait on gasoline range 5 according to congresswoman rosa delauro hamburger ground meat patty between two slices of bread was first created in america in 1900 by louis lassen danish immigrant owner of louis lunch in new haven connecticut 6 there have been rival claims by charlie nagreen frank charles menches oscar weber bilby fletcher davis 7 8 white castle traces origin of hamburger to hamburg germany with its invention by otto kuase 9 however it gained national recognition at 1904 st louis worlds fair when new york tribune referred to hamburger as innovation of food vendor on pike 8 no conclusive argument has ever ended dispute over invention article from abc news sums up one problem is that there is little written history another issue is that spread of burger happened largely at worlds fair from tiny vendors that came went in instant it is entirely possible that more than one person came up with idea at same time in different parts of country 10 claims of invention louis lassen louis lassen of louis lunch small lunch wagon in new haven conn\n",
      "answer_tokens: ['hamburger', 'wikipedia', 'photos', 'videos', 'hamburger', 'next', 'go', 'to', 'results', '51', '100', 'wikipedia', 'article', 'jump', 'to', 'navigation', 'search', 'this', 'article', 'is', 'about', 'sandwich', 'for', 'meat', 'served', 'as', 'part', 'of', 'such', 'sandwich', 'see', 'patty', 'for', 'other', 'uses', 'see', 'hamburger', 'disambiguation', 'hamburger', 'hamburger', 'on', 'roll', 'with', 'french', 'fries', 'course', 'media', 'hamburger', 'hamburger', 'cheeseburger', 'when', 'served', 'with', 'slice', 'of', 'cheese', 'is', 'sandwich', 'consisting', 'of', 'one', 'more', 'cooked', 'patties', 'of', 'ground', 'meat', 'usually', 'beef', 'placed', 'inside', 'sliced', 'bread', 'roll', 'bun', 'hamburgers', 'may', 'be', 'cooked', 'in', 'variety', 'of', 'ways', 'including', 'pan', 'frying', 'barbecuing', 'flame', 'broiling', 'hamburgers', 'are', 'often', 'served', 'with', 'cheese', 'lettuce', 'tomato', 'bacon', 'onion', 'pickles', 'condiments', 'such', 'as', 'mustard', 'mayonnaise', 'ketchup', 'relish', 'chiles', '1', 'term', 'burger', 'can', 'also', 'be', 'applied', 'to', 'meat', 'patty', 'on', 'its', 'own', 'especially', 'in', 'uk', 'where', 'term', 'patty', 'is', 'rarely', 'used', 'term', 'may', 'be', 'prefixed', 'with', 'type', 'of', 'meat', 'meat', 'substitute', 'used', 'as', 'in', 'turkey', 'burger', 'bison', 'burger', 'veggie', 'burger', 'hamburgers', 'are', 'sold', 'at', 'fast', 'food', 'restaurants', 'diners', 'specialty', 'high', 'end', 'restaurants', 'where', 'burgers', 'may', 'sell', 'for', 'several', 'times', 'cost', 'of', 'fast', 'food', 'burger', 'there', 'are', 'many', 'international', 'regional', 'variations', 'of', 'hamburger', 'contents', 'etymology', 'terminology', 'hamburger', 'is', 'named', 'after', 'hamburg', 'germany', 'term', 'hamburger', 'originally', 'derives', 'from', 'hamburg', '2', 'germany', 's', 'second', 'largest', 'city', 'in', 'german', 'burg', 'means', 'castle', 'fortified', 'settlement', 'fortified', 'refuge', 'is', 'widespread', 'component', 'of', 'place', 'names', 'first', 'element', 'of', 'name', 'is', 'perhaps', 'from', 'old', 'high', 'german', 'hamma', 'referring', 'to', 'bend', 'in', 'river', 'middle', 'high', 'german', 'hamme', 'referring', 'to', 'enclosed', 'area', 'of', 'pastureland', '3', 'hamburger', 'in', 'german', 'is', 'demonym', 'of', 'hamburg', 'similar', 'to', 'frankfurter', 'wiener', 'names', 'for', 'other', 'meat', 'based', 'foods', 'demonyms', 'of', 'cities', 'of', 'frankfurt', 'vienna', 'wien', 'respectively', 'term', 'burger', 'back', 'formation', 'is', 'associated', 'with', 'many', 'different', 'types', 'of', 'sandwiches', 'similar', 'to', 'ground', 'meat', 'hamburger', 'made', 'of', 'different', 'meats', 'such', 'as', 'buffalo', 'in', 'buffalo', 'burger', 'venison', 'kangaroo', 'turkey', 'elk', 'lamb', 'fish', 'like', 'salmon', 'in', 'salmon', 'burger', 'even', 'with', 'meatless', 'sandwiches', 'as', 'is', 'case', 'of', 'veggie', 'burger', '4', 'history', 'main', 'articles', 'history', 'of', 'hamburger', 'history', 'of', 'hamburger', 'in', 'united', 'states', 'there', 'have', 'been', 'many', 'claims', 'about', 'origin', 'of', 'hamburger', 'there', 'is', 'reference', 'to', 'hamburg', 'steak', 'as', 'early', 'as', '1884', 'in', 'boston', 'journal', 'oed', 'under', 'steak', 'on', 'july', '5', '1896', 'chicago', 'daily', 'tribune', 'made', 'highly', 'specific', 'claim', 'regarding', 'hamburger', 'sandwich', 'in', 'article', 'about', 'sandwich', 'car', 'distinguished', 'favorite', 'only', 'five', 'cents', 'is', 'hamburger', 'steak', 'sandwich', 'meat', 'for', 'which', 'is', 'kept', 'ready', 'in', 'small', 'patties', 'cooked', 'while', 'you', 'wait', 'on', 'gasoline', 'range', '5', 'according', 'to', 'congresswoman', 'rosa', 'delauro', 'hamburger', 'ground', 'meat', 'patty', 'between', 'two', 'slices', 'of', 'bread', 'was', 'first', 'created', 'in', 'america', 'in', '1900', 'by', 'louis', 'lassen', 'danish', 'immigrant', 'owner', 'of', 'louis', 'lunch', 'in', 'new', 'haven', 'connecticut', '6', 'there', 'have', 'been', 'rival', 'claims', 'by', 'charlie', 'nagreen', 'frank', 'charles', 'menches', 'oscar', 'weber', 'bilby', 'fletcher', 'davis', '7', '8', 'white', 'castle', 'traces', 'origin', 'of', 'hamburger', 'to', 'hamburg', 'germany', 'with', 'its', 'invention', 'by', 'otto', 'kuase', '9', 'however', 'it', 'gained', 'national', 'recognition', 'at', '1904', 'st', 'louis', 'worlds', 'fair', 'when', 'new', 'york', 'tribune', 'referred', 'to', 'hamburger', 'as', 'innovation', 'of', 'food', 'vendor', 'on', 'pike', '8', 'no', 'conclusive', 'argument', 'has', 'ever', 'ended', 'dispute', 'over', 'invention', 'article', 'from', 'abc', 'news', 'sums', 'up', 'one', 'problem', 'is', 'that', 'there', 'is', 'little', 'written', 'history', 'another', 'issue', 'is', 'that', 'spread', 'of', 'burger', 'happened', 'largely', 'at', 'worlds', 'fair', 'from', 'tiny', 'vendors', 'that', 'came', 'went', 'in', 'instant', 'it', 'is', 'entirely', 'possible', 'that', 'more', 'than', 'one', 'person', 'came', 'up', 'with', 'idea', 'at', 'same', 'time', 'in', 'different', 'parts', 'of', 'country', '10', 'claims', 'of', 'invention', 'louis', 'lassen', 'louis', 'lassen', 'of', 'louis', 'lunch', 'small', 'lunch', 'wagon', 'in', 'new', 'haven', 'conn']\n",
      "\n",
      "example: 3\n",
      "query: whales can be divided into two types toothed what \n",
      "query_tokens: ['whales', 'can', 'be', 'divided', 'into', 'two', 'types', 'toothed', 'what']\n",
      "answer_id: 14190\n",
      "answer: feeding whales feeding reproducing feeding whales can be divided into two types by way they feedbaleen whales mysticetes toothed whales odontecetes baleen whales are batch feedersthey use their plates of baleen to filter huge numbers of tiny prey out of water they can be divided into three groups according to how they filter their food rorquals such as brydes whale are gulpers they take in huge amounts of water with their prey their pleated throats billowing out to accommodate it then they force water out with their tongues straining food through mesh of baleen right whales skim feed they cruise through water with their enormous arched mouths open continuously filtering out prey gray whales are silt sifters they pump water sediment from seafloor through one side of their mouths across baleen out other toothed whales tend to eat individual animals such as squid fish in some cases other marine mammals these whales locate their prey by using echolocationlike human sonar they either seize their prey with their teeth suck them directly into their mouths swallow them whole \n",
      "answer_tokens: ['feeding', 'whales', 'feeding', 'reproducing', 'feeding', 'whales', 'can', 'be', 'divided', 'into', 'two', 'types', 'by', 'way', 'they', 'feedbaleen', 'whales', 'mysticetes', 'toothed', 'whales', 'odontecetes', 'baleen', 'whales', 'are', 'batch', 'feedersthey', 'use', 'their', 'plates', 'of', 'baleen', 'to', 'filter', 'huge', 'numbers', 'of', 'tiny', 'prey', 'out', 'of', 'water', 'they', 'can', 'be', 'divided', 'into', 'three', 'groups', 'according', 'to', 'how', 'they', 'filter', 'their', 'food', 'rorquals', 'such', 'as', 'brydes', 'whale', 'are', 'gulpers', 'they', 'take', 'in', 'huge', 'amounts', 'of', 'water', 'with', 'their', 'prey', 'their', 'pleated', 'throats', 'billowing', 'out', 'to', 'accommodate', 'it', 'then', 'they', 'force', 'water', 'out', 'with', 'their', 'tongues', 'straining', 'food', 'through', 'mesh', 'of', 'baleen', 'right', 'whales', 'skim', 'feed', 'they', 'cruise', 'through', 'water', 'with', 'their', 'enormous', 'arched', 'mouths', 'open', 'continuously', 'filtering', 'out', 'prey', 'gray', 'whales', 'are', 'silt', 'sifters', 'they', 'pump', 'water', 'sediment', 'from', 'seafloor', 'through', 'one', 'side', 'of', 'their', 'mouths', 'across', 'baleen', 'out', 'other', 'toothed', 'whales', 'tend', 'to', 'eat', 'individual', 'animals', 'such', 'as', 'squid', 'fish', 'in', 'some', 'cases', 'other', 'marine', 'mammals', 'these', 'whales', 'locate', 'their', 'prey', 'by', 'using', 'echolocationlike', 'human', 'sonar', 'they', 'either', 'seize', 'their', 'prey', 'with', 'their', 'teeth', 'suck', 'them', 'directly', 'into', 'their', 'mouths', 'swallow', 'them', 'whole']\n",
      "\n",
      "example: 4\n",
      "query: garnet comes in many colours what is commonest\n",
      "query_tokens: ['garnet', 'comes', 'in', 'many', 'colours', 'what', 'is', 'commonest']\n",
      "answer_id: 13325\n",
      "answer:  colors varieties of garnet colors varieties of garnet colors varieties of garnet published may 2010 by cara williams f g garnets are not just family they are more like clan they have much in common same crystal form cubic same chemical outline i say outline because in many ways chemistry varies between different types of garnets so while we call them family their dna varies enough to call them clan gemologists recognize six garnet families within garnet clan five are well known pyrope almandine spessartite grossular andradite sixth uvarovite is hardly ever seen heard from if you didnt catch name of your favorite hold on we will get to it in part 2 of this report besides these six gem garnets mineralogists know about more than 30 others some just theoretical yes there is lot we didnt know about that january birthstone read more about garnet in our gemopedia garnets adhere to three part formula there is silicate component as well as two other elements ratio of these components does not vary for more detail please look at chart below six garnet families tend to hang out in two groups aluminum group calcium group they occasionally mix intermarry not much aluminum calcium silicate do not cause color other elements do those wild genes aluminum garnets are called pyralspites word formed from individual names pyrope almandine spessartite calcium garnets are ugrandites uvarovite grossular andradite now imagine garnets as very old established clan while individuals might have certain last name they are not just jones smith they are combination of whatever their ancestors were so we never have pure pyrope pure almandine pyralspites pure pyrope would be colorless we all know there are no colorless garnets pyropes are always red getting their color from some almandine ancestry sometimes from bit of uvarovite bits of uvarovite genes molecules pop up in many garnets dna almandine is classic red garnet possibly most common type it has iron in its formula which creates color in this case strong red this is why it does not take much almandine in garnet to make it red spessartite is third pyralspite it has manganese in its formula this manganese creates yellowish orange color because many of spessartites ancestors married almandines many are reddish orange due to some iron lighter orange spessartites similar in color to orange soft drinks are almost pure spessartites ugrandites other group of gem garnets ugrandites is not as well known as plentiful as pyralspites uvarovite rich emerald green has chromium in its formula which stunts crystal growth this garnet is rarely seen as anything other than drusy lots of tiny crystals all close together however it leaves its mark on other garnets more than you might think grossular garnet has aluminum calcium in its formula neither of which creates color however grossulars can range widely in their shades are often close to pure they get their color from traces of other garnets mixed in andradite is last garnet is different from others in several ways it has iron in different state position within structure than almandine therefore it does not create red instead andradites can be golden brown even other colors if mixed garnet varieties part 2 i suspect many of garnet names we covered in part 1 may be new to some that is because various garnets are usually called by trade names that were created as different finds were discovered because no garnet is pure many are composed of combinations of three more garnets it is too confusing to stick to gemological names so we have always named these varieties on their appearance gemological properties origin regardless of what act\n",
      "answer_tokens: ['colors', 'varieties', 'of', 'garnet', 'colors', 'varieties', 'of', 'garnet', 'colors', 'varieties', 'of', 'garnet', 'published', 'may', '2010', 'by', 'cara', 'williams', 'f', 'g', 'garnets', 'are', 'not', 'just', 'family', 'they', 'are', 'more', 'like', 'clan', 'they', 'have', 'much', 'in', 'common', 'same', 'crystal', 'form', 'cubic', 'same', 'chemical', 'outline', 'i', 'say', 'outline', 'because', 'in', 'many', 'ways', 'chemistry', 'varies', 'between', 'different', 'types', 'of', 'garnets', 'so', 'while', 'we', 'call', 'them', 'family', 'their', 'dna', 'varies', 'enough', 'to', 'call', 'them', 'clan', 'gemologists', 'recognize', 'six', 'garnet', 'families', 'within', 'garnet', 'clan', 'five', 'are', 'well', 'known', 'pyrope', 'almandine', 'spessartite', 'grossular', 'andradite', 'sixth', 'uvarovite', 'is', 'hardly', 'ever', 'seen', 'heard', 'from', 'if', 'you', 'didnt', 'catch', 'name', 'of', 'your', 'favorite', 'hold', 'on', 'we', 'will', 'get', 'to', 'it', 'in', 'part', '2', 'of', 'this', 'report', 'besides', 'these', 'six', 'gem', 'garnets', 'mineralogists', 'know', 'about', 'more', 'than', '30', 'others', 'some', 'just', 'theoretical', 'yes', 'there', 'is', 'lot', 'we', 'didnt', 'know', 'about', 'that', 'january', 'birthstone', 'read', 'more', 'about', 'garnet', 'in', 'our', 'gemopedia', 'garnets', 'adhere', 'to', 'three', 'part', 'formula', 'there', 'is', 'silicate', 'component', 'as', 'well', 'as', 'two', 'other', 'elements', 'ratio', 'of', 'these', 'components', 'does', 'not', 'vary', 'for', 'more', 'detail', 'please', 'look', 'at', 'chart', 'below', 'six', 'garnet', 'families', 'tend', 'to', 'hang', 'out', 'in', 'two', 'groups', 'aluminum', 'group', 'calcium', 'group', 'they', 'occasionally', 'mix', 'intermarry', 'not', 'much', 'aluminum', 'calcium', 'silicate', 'do', 'not', 'cause', 'color', 'other', 'elements', 'do', 'those', 'wild', 'genes', 'aluminum', 'garnets', 'are', 'called', 'pyralspites', 'word', 'formed', 'from', 'individual', 'names', 'pyrope', 'almandine', 'spessartite', 'calcium', 'garnets', 'are', 'ugrandites', 'uvarovite', 'grossular', 'andradite', 'now', 'imagine', 'garnets', 'as', 'very', 'old', 'established', 'clan', 'while', 'individuals', 'might', 'have', 'certain', 'last', 'name', 'they', 'are', 'not', 'just', 'jones', 'smith', 'they', 'are', 'combination', 'of', 'whatever', 'their', 'ancestors', 'were', 'so', 'we', 'never', 'have', 'pure', 'pyrope', 'pure', 'almandine', 'pyralspites', 'pure', 'pyrope', 'would', 'be', 'colorless', 'we', 'all', 'know', 'there', 'are', 'no', 'colorless', 'garnets', 'pyropes', 'are', 'always', 'red', 'getting', 'their', 'color', 'from', 'some', 'almandine', 'ancestry', 'sometimes', 'from', 'bit', 'of', 'uvarovite', 'bits', 'of', 'uvarovite', 'genes', 'molecules', 'pop', 'up', 'in', 'many', 'garnets', 'dna', 'almandine', 'is', 'classic', 'red', 'garnet', 'possibly', 'most', 'common', 'type', 'it', 'has', 'iron', 'in', 'its', 'formula', 'which', 'creates', 'color', 'in', 'this', 'case', 'strong', 'red', 'this', 'is', 'why', 'it', 'does', 'not', 'take', 'much', 'almandine', 'in', 'garnet', 'to', 'make', 'it', 'red', 'spessartite', 'is', 'third', 'pyralspite', 'it', 'has', 'manganese', 'in', 'its', 'formula', 'this', 'manganese', 'creates', 'yellowish', 'orange', 'color', 'because', 'many', 'of', 'spessartites', 'ancestors', 'married', 'almandines', 'many', 'are', 'reddish', 'orange', 'due', 'to', 'some', 'iron', 'lighter', 'orange', 'spessartites', 'similar', 'in', 'color', 'to', 'orange', 'soft', 'drinks', 'are', 'almost', 'pure', 'spessartites', 'ugrandites', 'other', 'group', 'of', 'gem', 'garnets', 'ugrandites', 'is', 'not', 'as', 'well', 'known', 'as', 'plentiful', 'as', 'pyralspites', 'uvarovite', 'rich', 'emerald', 'green', 'has', 'chromium', 'in', 'its', 'formula', 'which', 'stunts', 'crystal', 'growth', 'this', 'garnet', 'is', 'rarely', 'seen', 'as', 'anything', 'other', 'than', 'drusy', 'lots', 'of', 'tiny', 'crystals', 'all', 'close', 'together', 'however', 'it', 'leaves', 'its', 'mark', 'on', 'other', 'garnets', 'more', 'than', 'you', 'might', 'think', 'grossular', 'garnet', 'has', 'aluminum', 'calcium', 'in', 'its', 'formula', 'neither', 'of', 'which', 'creates', 'color', 'however', 'grossulars', 'can', 'range', 'widely', 'in', 'their', 'shades', 'are', 'often', 'close', 'to', 'pure', 'they', 'get', 'their', 'color', 'from', 'traces', 'of', 'other', 'garnets', 'mixed', 'in', 'andradite', 'is', 'last', 'garnet', 'is', 'different', 'from', 'others', 'in', 'several', 'ways', 'it', 'has', 'iron', 'in', 'different', 'state', 'position', 'within', 'structure', 'than', 'almandine', 'therefore', 'it', 'does', 'not', 'create', 'red', 'instead', 'andradites', 'can', 'be', 'golden', 'brown', 'even', 'other', 'colors', 'if', 'mixed', 'garnet', 'varieties', 'part', '2', 'i', 'suspect', 'many', 'of', 'garnet', 'names', 'we', 'covered', 'in', 'part', '1', 'may', 'be', 'new', 'to', 'some', 'that', 'is', 'because', 'various', 'garnets', 'are', 'usually', 'called', 'by', 'trade', 'names', 'that', 'were', 'created', 'as', 'different', 'finds', 'were', 'discovered', 'because', 'no', 'garnet', 'is', 'pure', 'many', 'are', 'composed', 'of', 'combinations', 'of', 'three', 'more', 'garnets', 'it', 'is', 'too', 'confusing', 'to', 'stick', 'to', 'gemological', 'names', 'so', 'we', 'have', 'always', 'named', 'these', 'varieties', 'on', 'their', 'appearance', 'gemological', 'properties', 'origin', 'regardless', 'of', 'what', 'act']\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice the difference in the types of the different structures we use. Run the following cell to check the types. Do they make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:19:52.400667471Z",
     "start_time": "2026-02-11T12:19:52.372448348Z"
    }
   },
   "source": [
    "#type of original dataset\n",
    "print(type(dataset))\n",
    "print(\"--\")\n",
    "#type of the split of the dataset\n",
    "print(type(dataset['test']))\n",
    "print(\"--\")\n",
    "#type of original query\n",
    "print(dataset['train'][0]['query'])\n",
    "print(type(dataset['train'][0]['query']))\n",
    "print(\"--\")\n",
    "#type of tokenized query\n",
    "print(dataset['train'][0]['query_tokens'])\n",
    "print(type(dataset['train'][0]['query_tokens']))\n",
    "print(\"--\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "--\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "--\n",
      " fictional character norville rogers is better known by nickname \n",
      "<class 'str'>\n",
      "--\n",
      "['fictional', 'character', 'norville', 'rogers', 'is', 'better', 'known', 'by', 'nickname']\n",
      "<class 'list'>\n",
      "--\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "In this section you will built a bag-of-words representation of the dataset. We will use numpy arrays to store the results. The bag-of-words representation is a simple and effective way to represent text data. It involves creating a vocabulary of unique words from the dataset and representing each sentence as a vector of word counts. We first need the vocabulary, which we will build from both the full sentences and the compressed sentences. Similar to the first lab, the vocabulary will be a list of unique words from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extracting Vocabulary\n",
    "\n",
    "<a name='e4'></a>\n",
    "#### Exercise 4: Extracting vocabulary counts\n",
    "\n",
    "In the following cell, you will implement a function that takes two datasets (`dataset`, and `answers_dataset`) and returns a dictionary with the counts of each word in the vocabulary. The dictionary should be of the form {word: count}. As in previous lab, you will use the `Counter` class from the `collections` module to do this. Iterate over the two datasets and count the tokens in `query_tokens` and `answer_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:19:52.430680015Z",
     "start_time": "2026-02-11T12:19:52.404092290Z"
    }
   },
   "source": [
    "def extract_vocabulary_counts(dataset, answers_dataset):\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from the tokenized sentences\n",
    "    Args:\n",
    "        dataset: a Dataset from which 'query_tokens' are used to build vocabulary\n",
    "        answers_dataset: a Dataset from which 'answer_tokens' are used to build vocabulary\n",
    "\n",
    "    Returns: a Counter object with the counts of each word in the vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = Counter()\n",
    "    ### YOUR CODE HERE\n",
    "    for example in dataset:\n",
    "        vocab.update(example['query_tokens'])\n",
    "\n",
    "    for example in answers_dataset:\n",
    "        vocab.update(example['answer_tokens'])\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return vocab"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we use the function you implemented. Notice that we build our vocabulary based on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:07.918595949Z",
     "start_time": "2026-02-11T12:19:52.433445585Z"
    }
   },
   "source": [
    "vocab_counter = extract_vocabulary_counts(dataset['train'], answers_dataset['train'])\n",
    "print(len(vocab_counter))\n",
    "print(vocab_counter.most_common(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382356\n",
      "[('of', 830174), ('in', 598923), ('to', 561393), ('is', 289430), ('was', 228546), ('for', 211141), ('on', 186211), ('as', 183060), ('that', 175038), ('with', 171094)]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will truncate the vocabulary. We also create the handy `token_to_id` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.260447313Z",
     "start_time": "2026-02-11T12:20:08.201261215Z"
    }
   },
   "source": [
    "max_vocab_size = 20_000\n",
    "vocab = vocab_counter.most_common(max_vocab_size)\n",
    "# cast to list of words\n",
    "vocab = [word for word, _ in vocab]\n",
    "token_to_id = {word: i for i, word in enumerate(vocab)}"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Implementation\n",
    "\n",
    "\n",
    "<a name='e5'></a>\n",
    "#### Exercise 5: Bag of Words\n",
    "Here we will create the bag-of-words representation of the sentences. The function will take a single sentence (list of tokens) and return an array of size `vocab_size` with the counts of each word in the vocabulary. The\n",
    "`vocab_size` is calculated as the length of the passed `token_to_id` dictionary. The resulting array should have zeros everywhere but the indices corresponding to the words in the vocabulary where it should have the counts of the words in the sentence. For example, if the sentence is `['fox', 'and', 'deer']` and the vocabulary is `{'fox': 0, 'and': 1, 'deer': 2}`, the resulting array should be `[1, 1, 1]`. If the sentence is `['fox', 'and', 'fox', 'deer']`, the resulting array should be `[2, 1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.273952891Z",
     "start_time": "2026-02-11T12:20:08.265184233Z"
    }
   },
   "source": [
    "def bag_of_words(sentence_tokens, token_to_id):\n",
    "    \"\"\"\n",
    "    Creates a bag-of-words representation of the sentence\n",
    "    Args:\n",
    "        sentence_tokens: a list of tokens\n",
    "        token_to_id: a dictionary mapping each word to an index in the vocabulary\n",
    "\n",
    "    Returns:: a numpy array of size vocab_size with the counts of each word in the vocabulary\n",
    "    \"\"\"\n",
    "    vocab_size = len(token_to_id)\n",
    "    bow = np.zeros(vocab_size, dtype=int)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    bow = np.array([sentence_tokens.count(word) for word in vocab])\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return bow"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's test the function. The output should be a numpy array of size `vocab_size` with the counts of each word in the vocabulary. Notice that most of the elements of the BOW representation are zeros."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.295877580Z",
     "start_time": "2026-02-11T12:20:08.275080893Z"
    }
   },
   "source": [
    "print('Tokenized sentence:')\n",
    "print(dataset['test'][0]['query_tokens'])\n",
    "query_bow = bag_of_words(dataset['test'][0]['query_tokens'], token_to_id)\n",
    "query_non_zero_bow = np.nonzero(query_bow)[0]\n",
    "\n",
    "print('Bag of words:')\n",
    "print(query_bow)\n",
    "print('Type of bag of words:')\n",
    "print(type(query_bow))\n",
    "print('Shape of bag of words:')\n",
    "print(query_bow.shape)\n",
    "print('Non-zero elements in bag of words:')\n",
    "print(query_non_zero_bow)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence:\n",
      "['which', 'moon', 'of', 'solar', 'system', 'is', 'named', 'after', 'greek', 'god', 'of', 'fear']\n",
      "Bag of words:\n",
      "[2 0 0 ... 0 0 0]\n",
      "Type of bag of words:\n",
      "<class 'numpy.ndarray'>\n",
      "Shape of bag of words:\n",
      "(20000,)\n",
      "Non-zero elements in bag of words:\n",
      "[   0    3   18   39  258  295  394  401  925  970 2026]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's examine further the non-zero elements:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.356910317Z",
     "start_time": "2026-02-11T12:20:08.317452256Z"
    }
   },
   "source": [
    "print('Non-zero elements in bag of words:')\n",
    "print(query_non_zero_bow)\n",
    "for i in query_non_zero_bow:\n",
    "    print(vocab[i], ':', query_bow[i])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero elements in bag of words:\n",
      "[   0    3   18   39  258  295  394  401  925  970 2026]\n",
      "of : 2\n",
      "is : 1\n",
      "which : 1\n",
      "after : 1\n",
      "named : 1\n",
      "system : 1\n",
      "god : 1\n",
      "greek : 1\n",
      "moon : 1\n",
      "fear : 1\n",
      "solar : 1\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Function for Embedding Text\n",
    "\n",
    "The following function will apply all the steps we implemented to a single sentence. It returns a bag of words representation that we will use to calculate the similarity between different sentences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.400345794Z",
     "start_time": "2026-02-11T12:20:08.363150525Z"
    }
   },
   "source": [
    "def embed_text(text, clean_fn, tokenize_fn, embed_fn):\n",
    "    \"\"\"\n",
    "    Embeds the text using the provided functions. The pipeline applies cleaning (clean_fn), tokenization (tokenize_fn), and embedding (embed_fn).\n",
    "    Args:\n",
    "        text: the text to be embedded\n",
    "        clean_fn: function/Callable clean_fn(text:str):str\n",
    "        tokenize_fn: function/Callable tokenize_fn(text:str): List[str]\n",
    "        embed_fn: function/Callable embed_fn(tokens:List[str]): np.ndarray\n",
    "\n",
    "    Returns: the embedding of the text as a numpy array\n",
    "    \"\"\"\n",
    "    cleaned = clean_fn(text)\n",
    "    tokens = tokenize_fn(cleaned)\n",
    "    embedding = embed_fn(tokens)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "embedding = embed_text(\"This is an example of a sentence\", clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "print(embedding.shape)\n",
    "print(np.nonzero(embedding)[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000,)\n",
      "[   0    3   17  486 3507]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "<a name='e6'></a>\n",
    "#### Exercise 6: Cosine Similarity between two vectors\n",
    "\n",
    "Complete the following function that given any two vectors will compute the cosine similarity. If you don't remember the formula for the cosine similarity, revisit the course material. Notice that the function receives numpy arrays and recall that you can express cosine similarity as a dot product. Use numpy functions to write an efficient implementation. Two more exercises builds upon this one, so make sure to understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.413117906Z",
     "start_time": "2026-02-11T12:20:08.402485569Z"
    }
   },
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors\n",
    "    Args:\n",
    "        vector1: numpy array of the first vector\n",
    "        vector2: numpy array of the second vector\n",
    "\n",
    "    Returns: cosine similarity\n",
    "\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    numerator = np.dot(vector1, vector2) # dot product or vectors 1 and 2\n",
    "    denominator = np.linalg.norm(vector1) * np.linalg.norm(vector2) # product of the norms (magnitudes) of the vectors\n",
    "    cosine_sim = numerator / denominator # cosine similarity formula\n",
    "\n",
    "    return cosine_sim\n",
    "    ### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.453025997Z",
     "start_time": "2026-02-11T12:20:08.415834780Z"
    }
   },
   "source": [
    "cosine_similarity(np.array([0, 1, 2]), np.array([0, 2, 4]))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9999999999999998)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see how similar are the BOW representations of some sentences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:20:08.587471702Z",
     "start_time": "2026-02-11T12:20:08.455664494Z"
    }
   },
   "source": [
    "sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog.',\n",
    "    'Some interesting document containing sentences.',\n",
    "    'The quick brown fox jumps over the lazy cat and some other stuff.',\n",
    "    'Fox and deer are not friends.',\n",
    "    'Fox and deer are not friends. But this document is a lot longer than the previous one. We can add sentence by sentence and see how the embeddings change.',\n",
    "]\n",
    "embedded_sentences = [\n",
    "    embed_text(sentence, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "    for sentence in sentences\n",
    "]\n",
    "\n",
    "query = 'fox and deer'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "\n",
    "cosine_similarities = [\n",
    "    cosine_similarity(embedded_query, embedded_sentence)\n",
    "    for embedded_sentence in embedded_sentences\n",
    "]\n",
    "print(f'Query: {query}')\n",
    "for sent, cos_sim in zip(sentences, cosine_similarities):\n",
    "    print(f'Cosine Similarity: {cos_sim:.4f} - Sentence: {sent}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: fox and deer\n",
      "Cosine Similarity: 0.2673 - Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Cosine Similarity: 0.0000 - Sentence: Some interesting document containing sentences.\n",
      "Cosine Similarity: 0.2236 - Sentence: The quick brown fox jumps over the lazy cat and some other stuff.\n",
      "Cosine Similarity: 0.6325 - Sentence: Fox and deer are not friends.\n",
      "Cosine Similarity: 0.2887 - Sentence: Fox and deer are not friends. But this document is a lot longer than the previous one. We can add sentence by sentence and see how the embeddings change.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Retrieval\n",
    "\n",
    "In this section, we will use the BOW representations to finally search for the answers to our questions. We start by calculating the BOWs of queries and answers of the whole `validation` subset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:30:38.929112744Z",
     "start_time": "2026-02-11T12:20:08.589443765Z"
    }
   },
   "source": [
    "valid_queries_bows = []\n",
    "for example in tqdm.tqdm(dataset['validation']):\n",
    "    valid_queries_bows.append(bag_of_words(example['query_tokens'], token_to_id))\n",
    "\n",
    "valid_answers_bows = []\n",
    "for example in tqdm.tqdm(answers_dataset['validation']):\n",
    "    valid_answers_bows.append(bag_of_words(example['answer_tokens'], token_to_id))\n",
    "\n",
    "valid_queries_bows = np.array(valid_queries_bows)\n",
    "valid_answers_bows = np.array(valid_answers_bows)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:25<00:00, 392.91it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9756/9756 [10:03<00:00, 16.18it/s]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e7'></a>\n",
    "#### Exercise 7: Cosine Similarity between a vector and an array of vectors\n",
    "\n",
    "The next step in our retrieval system, would be to calculate the proximity of a query to our retrieval corpus (in our case that is all the sentences).\n",
    "\n",
    "Complete the following function to calculate the cosine similarity between a vector (first parameter `vector`, that will usually be the query vector) and all other vectors (second parameter `other_vectors`, that will be the sentence embeddings in our case). Note that the `other_vectors` parameter is a single numpy array of size `N x D`, where $N$ is the number of vectors and $D$ is the dimension of each vector.\n",
    "\n",
    "For maximum efficiency (we will need it) do not use loops. Try to write the implementation with numpy functions. Hint: matrix multiplication can be seen as calculating the dot product between rows and columns of the multiplied matrices."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:30:39.241612531Z",
     "start_time": "2026-02-11T12:30:39.207283245Z"
    }
   },
   "source": [
    "def cosine_similarity_1_to_n(vector, other_vectors):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a single vector and other vectors.\n",
    "    Args:\n",
    "        vector: a numpy array representing a vector of D dimensions\n",
    "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
    "\n",
    "    Returns: a 1D numpy array of size N containing the cosine similarity between the vector and all the other vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # compute the dot product of the vector with all other vectors\n",
    "    numerator = np.dot(other_vectors, vector)\n",
    "\n",
    "    # compute the  norm (magnitude) of the single vector\n",
    "    norm_vector = np.linalg.norm(vector)\n",
    "\n",
    "    # compute the norm (magnitude) of each vector in the array (along the columns axis)\n",
    "    norm_others = np.linalg.norm(other_vectors, axis=1)\n",
    "\n",
    "    denominator = norm_vector * norm_others\n",
    "\n",
    "    # cosine similarity formula\n",
    "    cos_sim = numerator / denominator\n",
    "\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now can try out our retrieval system by calculating the cosine similarities between the query and all answers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:30:41.774397354Z",
     "start_time": "2026-02-11T12:30:39.244497091Z"
    }
   },
   "source": [
    "query = 'Which vegetable is Blackadder‚Äôs servant obsessed with in the UK television series ‚ÄòBlackadder II‚Äô?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "print(query_similarity.shape)\n",
    "print(query_similarity[:10])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9756,)\n",
      "[0.2951201  0.18254261 0.20326841 0.06305416 0.11748907 0.20971379\n",
      " 0.05455447 0.18774223 0.25172953 0.18134958]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:30:42.169775273Z",
     "start_time": "2026-02-11T12:30:42.086534828Z"
    }
   },
   "source": [
    "most_similar = int(np.argmax(query_similarity))\n",
    "print(most_similar)\n",
    "print(query_similarity[most_similar])\n",
    "print(valid_answers_bows[most_similar])\n",
    "print(answers_dataset['validation'][most_similar]['answer'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4774\n",
      "0.425795841107109\n",
      "[23 26  6 ...  0  0  0]\n",
      "learn talk about blackadder 1980s british television series 1983 british television programme debuts bbc television sitcoms blackadder website blackadder is series of four bbc1 period british sitcoms along with several one off installments all television episodes starred rowan atkinson as anti hero edmund blackadder tony robinson as blackadders dogsbody baldrick each series was set in different historical period with two protagonists accompanied by different characters though several reappear in one series another for example melchett stephen fry lord flashheart rik mayall first series black adder was written by richard curtis rowan atkinson while subsequent episodes were written by curtis ben elton shows were produced by john lloyd in 2000 fourth series blackadder goes forth ranked at 16 in 100 greatest british television programmes list created by british film institute also in 2004 tv poll to find britains best sitcom blackadder was voted second best british sitcom of all time topped by only fools horses it was also ranked as 20th best tv show of all time by empire magazine 1 contents premise edit although each series is set in different era all follow misfortunes of edmund blackadder played by atkinson who in each is member of british family dynasty present at many significant periods places in british history it is implied in each series that blackadder character is descendant of previous one end theme lyrics of series 2 episode heads specify that he is great grandson of previous although it is never specified how when any of blackadders who are usually single not in relationship managed to father children 2 as generations progress each blackadder becomes increasingly clever perceptive while familys social status steadily erodes however each blackadder remains cynical cowardly opportunist maintaining increasing his own status fortunes regardless of his surroundings life of each blackadder is also entwined with his servant each from baldrick family line played by tony robinson each generation acts as dogsbody to his respective blackadder they decrease in intelligence in personal hygiene standards as their masters intellect increases each blackadder baldrick is also saddled with tolerating presence of dim witted aristocrat this role was taken in first two series by lord percy percy played by tim mcinnerny with hugh laurie playing role in third fourth series as prince george prince regent lieutenant george respectively each series was set in different period of british history beginning in 1485 ending in 1917 comprised six half hour episodes first series made in 1983 was called black adder was set in fictional reign of richard iv second series blackadder ii 1986 was set during reign of elizabeth i blackadder third 1987 was set during late 18th early 19th centuries in reign of george iii blackadder goes forth 1989 was set in 1917 in trenches of great war series specials edit main article black adder black adder first series of blackadder was written by richard curtis rowan atkinson produced by john lloyd it originally aired on bbc1 from 15 june 1983 to 20 july 1983 was joint production with australian seven network set in 1485 at end of british middle ages series is written as alternative history in which king richard iii won battle of bosworth field only to be mistaken for someone else murdered is succeeded by richard iv brian blessed one of princes in tower series follows exploits of richard ivs unfavoured second son edmund duke of edinburgh who calls himself black adder in his various attempts to increase his standing with\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function returns the indices of the top-k elements in the array. If the `sorted` parameter is `True` (it is by default) the returned array will be sorted in the descending order (of the corresponding values in array). For example, if the `array` is `[3, 2, 4, 1]` and `k=2` the returned numpy array will be `[2, 0]` if `sorted` is True (the top values are `3` and `4` with indices `0` and `2`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:30:42.185529256Z",
     "start_time": "2026-02-11T12:30:42.173625587Z"
    }
   },
   "source": [
    "def top_k_indices(array, k, sorted=True):\n",
    "    \"\"\"\n",
    "    Returns top-k indices from the 1D array. If `sorted` is `True` the returned indices are sorted in the descending order\n",
    "    Args:\n",
    "        array: a 1D numpy array\n",
    "        k: a number of top indices to return\n",
    "        sorted: if True, the returned indices are sorted in descending order\n",
    "\n",
    "    Returns: a 1D numpy array containing top-k indices\n",
    "\n",
    "    \"\"\"\n",
    "    top_k = np.argpartition(array, -k)[-k:]\n",
    "    if sorted:\n",
    "        selected = array[top_k]\n",
    "        sorted_selected = (-selected).argsort()\n",
    "        top_k = top_k[sorted_selected]\n",
    "    return top_k\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:30:42.224283603Z",
     "start_time": "2026-02-11T12:30:42.189441917Z"
    }
   },
   "source": [
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "for idx in top_indices:\n",
    "    print(answers_dataset['validation'][idx]['answer'])\n",
    "    print(f'similarity: {query_similarity[idx]}')\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn talk about blackadder 1980s british television series 1983 british television programme debuts bbc television sitcoms blackadder website blackadder is series of four bbc1 period british sitcoms along with several one off installments all television episodes starred rowan atkinson as anti hero edmund blackadder tony robinson as blackadders dogsbody baldrick each series was set in different historical period with two protagonists accompanied by different characters though several reappear in one series another for example melchett stephen fry lord flashheart rik mayall first series black adder was written by richard curtis rowan atkinson while subsequent episodes were written by curtis ben elton shows were produced by john lloyd in 2000 fourth series blackadder goes forth ranked at 16 in 100 greatest british television programmes list created by british film institute also in 2004 tv poll to find britains best sitcom blackadder was voted second best british sitcom of all time topped by only fools horses it was also ranked as 20th best tv show of all time by empire magazine 1 contents premise edit although each series is set in different era all follow misfortunes of edmund blackadder played by atkinson who in each is member of british family dynasty present at many significant periods places in british history it is implied in each series that blackadder character is descendant of previous one end theme lyrics of series 2 episode heads specify that he is great grandson of previous although it is never specified how when any of blackadders who are usually single not in relationship managed to father children 2 as generations progress each blackadder becomes increasingly clever perceptive while familys social status steadily erodes however each blackadder remains cynical cowardly opportunist maintaining increasing his own status fortunes regardless of his surroundings life of each blackadder is also entwined with his servant each from baldrick family line played by tony robinson each generation acts as dogsbody to his respective blackadder they decrease in intelligence in personal hygiene standards as their masters intellect increases each blackadder baldrick is also saddled with tolerating presence of dim witted aristocrat this role was taken in first two series by lord percy percy played by tim mcinnerny with hugh laurie playing role in third fourth series as prince george prince regent lieutenant george respectively each series was set in different period of british history beginning in 1485 ending in 1917 comprised six half hour episodes first series made in 1983 was called black adder was set in fictional reign of richard iv second series blackadder ii 1986 was set during reign of elizabeth i blackadder third 1987 was set during late 18th early 19th centuries in reign of george iii blackadder goes forth 1989 was set in 1917 in trenches of great war series specials edit main article black adder black adder first series of blackadder was written by richard curtis rowan atkinson produced by john lloyd it originally aired on bbc1 from 15 june 1983 to 20 july 1983 was joint production with australian seven network set in 1485 at end of british middle ages series is written as alternative history in which king richard iii won battle of bosworth field only to be mistaken for someone else murdered is succeeded by richard iv brian blessed one of princes in tower series follows exploits of richard ivs unfavoured second son edmund duke of edinburgh who calls himself black adder in his various attempts to increase his standing with\n",
      "similarity: 0.425795841107109\n",
      "\n",
      "my questions documents documents share my questions embed iframe src http docslide usembedmy questions html width 750 height 600 frameborder 0 marginwidth 0 marginheight 0 scrolling no style border 1px solid ccc border width 1px margin bottom 5px max width 100 allowfullscreen iframe div style margin bottom 5px strong href http docslide usdocumentsmy questions html title my questions target _blank my questionsdiv sizepx download my questions transcript chemically pure gold contains how many carats what is tallest thickest type of grass what was surname of family who employed julie andrews character in sound of music which nation has won eurovision song contest more than any other what is most common gas in air we breathe which three different actors played batman in movies between 1989 1997 what colour is barts skateboard in introduction theme tune to which tv show starts with line stick pony in me pocket which soap opera is set in fictional county of borsetshire who did sue barker replace as host of bbc quiz show question of sport which generation game presenter was famous for his catchphrase shut that door no mean city by maggie bell is theme tune to which long running scottish tv detective show anthony barbara dave denise jim norma make up which famous family on british tv which part did deforest kelley play in tv series star trek true false in space it is impossible to cry famous sitcom actor kelsey grammar provides voice for for character in which famous cartoon tv series largest ever picnic for childs toy was held in dublin in 1995 where 33573 of toys were there what was toy which american state comes first alphabetically in greek legend what is name given to creature that is half man half bull which country has airline klm sinking of which famous german battleship was portrayed in title of 1960 film what organisation is also known as la cosa nostra what was titanics first port of call after it left southampton which mountain overshadows fort william in scotland what was name of 1995 film starring sandra bullock as computer expert whose identity is erased penguin called wheezy was character in which film who played vince in 1980s tv series just good friends in which 1994 film did whoopi goldberg provide voice of hyena called shenzi what is only venomous snake in britain how many pieces are there in standard set of dominoes james earl ray was responsible for whos death in 1968 in which city in england is national railway museum in music world which group sacked simon fuller in 1997 which roman god is one of symbols of st valentines day what was challanging method of catching fly asked of daniel in film karate kid actor richard kiel is best known for playing which character in two bond films which is odd one out comet dixon cupid vixen which planet in solar system is named after roman messenger to gods what product did coke invented in 1982 which japanese word also used in english language means empty orchestra on which date does halloween fall oscar is first name of which of famous songwriting duo rogers hammerstein 24 bamboo von trappe ireland nitrogen michael keaton val kilmer george clooney green only fools horses archers david coleman larry grayson taggart royle family dr leonard bones mccoy true there is no gravity so tears cannot flow simpsons teddy bear alabama minotaur netherlands bismark mafia cherbourg ben nevis net toy story 2 paul nicholas lion king adder 28 martin luther kings york manager of spice girls cupid using chopsticks to do it jaws in two james bond films dixon others are santas reindeer mercury diet coke karaoke october 31st hammerstein on 11th february 1990 which fam\n",
      "similarity: 0.3847884245337681\n",
      "\n",
      "2001 ko final february which ex pm was awarded earldom on his 90th birthday harold macmillan b1 member of house of lords ex mp who celebrated his 100th birthday in november 1984 mannie shinwell which government department banned trades unions causing national outcry gchq government communications headquarters outside which foreign government building was policewoman yvonne fletcher shot fatally wounded libyan peoples bureau libyan embassy a3 in course of violent argument in april which recording artist was shot killed by his father marvin gaye in october who was killed by members of her own bodyguard indira ghandi a4 in march british government announced its approval of sale of which shipyard on lower clyde to trafalgar house scott lithgow b4 in october which bank bullion dealer was rescued from debts of around √Ø¬Ω250 million by bank of england buy out johnson matthey subject √Ø¬Ωone word cinema√Ø¬Ω answers a1 1992 oscar winning clint eastwood film in which former hired killer turned unsuccessful farmer returns to his old ways in pursuit of 1000 reward unforgiven b1 1972 john boorman film in which leading character played by ned beatty is raped by √Ø¬Ωhillbilly√Ø¬Ω deliverance a2 1929 film hitchcock√Ø¬Ωs first talkie in which scotland yard inspector is placed in difficult position when he discovers his girlfriend has committed murder blackmail b2 set in rio 1946 hitchcock film with cary grant ingrid bergman in which woman marries nazi renegade to help us government notorious a3 1916 film by d w griffith starring lillian gish in one of four intercut stories including balshazzar√Ø¬Ωs feast st bartholomew√Ø¬Ωs day massacre intolerance b3 1967 camped up version of faust in which short order cook is saved from suicide by mr spiggott who offers him 7 wishes in exchange for his soul bedazzled a4 1924 erich von stroheim film in which ex miner turned dentist kills his avaricious wife her lover greed b4 set in mid 19th century 1999 film starring guy pearce robert carlyle in which cannibalistic officer commands isolated army outpost ravenous answers a1 liqueur cura√Ø¬Ωao say √Ø¬Ωkoor sow√Ø¬Ω is traditionally flavoured with sugar which fruit orange b1 which spirit takes its name from place near guadalajara say √Ø¬Ωgwadlahara√Ø¬Ω where conquistadors first developed it from variety of aztec drink tequila a2 with peculiar agreeable taste which coarse potent liquor is made in east indies from variety of sources including fermented rice coconut juice arrack b2 used to season food fruit as well as alcoholic drinks which flavouring is prepared with oil distilled from aromatic bark of two s american trees blended with herbs bears former name of port in venezuela angostura now called cuidad bolivar a3 derived from town in north east hungary what name is shared by grape variety golden yellow coloured sweet aromatic wine tokay from tokaj subject wordgame √Ø¬Ωno√Ø¬Ω as in √Ø¬Ωnote√Ø¬Ω answers √Ø¬Ω spout on hose etc from which jet issues nozzel √Ø¬Ω small round piece of meat chocolate made with hazelnuts noisette √Ø¬Ω something someone absolutely un\n",
      "similarity: 0.3570604012144514\n",
      "\n",
      "macclesfield pub quiz league may 2008 macclesfield pub quiz league wednesday may 28 2008 final for cup 1 which american poet wrote lines candy is dandy liquor is quicker later added lines pot is not ogden nash original from 1931 last lines added by nash in 1968 2 which archipelagic nation consists of over 7000 islands largest of which are luzon mindanao philippines 7107 islands to be precise 3 which london theatre has same name as silvery white metal with atomic number 46 palladium 4 which murderer used false name john robinson whilst trying to escape to quebec on ss montrose with his mistress dr crippen after murder of his wife cora 5 what is name of mp for crewe nantwich who died in april 2008 gwyneth dunwoody 6 complete monopoly set piccadilly coventry street leicester square 7 which major new york street intersects with broadway at times square 42nd street 8 which composer is buried adjacent to organ in westminster abbey henry purcell 9 if you were reading book published by fodors what would subject be travel worlds largest english language publisher of travel tourism info 10 two famous people met at ujiji near shore of lake tanganyika in tanzania on 10th of november 1871 name either sir henry morton stanley dr david livingstone dr livingstone i presume 11 which office currently held by martin rees has also been held by john flamsteed edmond halley astronomer royal flamsteed was first in 1675 was followed by halley 12 what is name of ships captain who is subject of herman wouks novel later film caine mutiny captain queeg 13 at end of which famous race is slowest finisher awarded title lanterne rouge red lantern tour de france it refers to red lights on last carriage of train which indicate that no wagon has been lost 14 in norse mythology who what is yggdrasil tree of world great ash tree that connects norse cosmos together tree is what we are after 15 which american singer known as cry guy nabob of sob had number 1 uk hit with just walking in rain in 1956 johnnie ray 16 which american artist whose most famous works were numbered rather than named died at age of 44 when he crashed his car whilst drunk in new york in 1956 jackson pollock 17 what is name of strait between india sri lanka palk strait 18 what word is used to describe person who dies without having made will intestate do not accept dead 19 what name is given to series of pictures apparently made by random ink blots which is used as means of psychological testing rorschach test 20 novel oil by upton sinclair was inspiration for successful 2007 film featuring leading character called daniel plainview what is films title there will be blood daniel day lewis stars as plainview 21 which pope died in 1978 only 33 days after being elected john paul i first 22 which game bird found extensively in scotland is known as snow grouse ptarmigan 23 which english football league team has been managed by trevor francis david pleat ron atkinson sheffield wednesday 24 when neil armstrong buzz aldrin walked on moon who stayed up in space michael collins command module pilot of apollo 11 25 refreshment sunday is fourth sunday in lent by what name is it more commonly known mothering sunday mothers day of course 26 william webb ellis won much prized blue for oxford university at which sport cricket played v cambridge in 1827 27 in greek mythology leto is mother of twins by zeus name either of them apollo artemis 28 in which shakespeare play is viola heroine twelfth night 29 leona lewis has recently topped us charts with her single bleeding love \n",
      "similarity: 0.35195473003325534\n",
      "\n",
      "danger mouse heroes wiki fandom powered by wikia his sidekick penfold his boss colonel k professor heinrich von squawkencluck professor squawkencluck jeopardy mouse enemies baron silas greenback stiletto count duckula doctor augustus p crumhorn snowman princess quarrk danger mouse is worlds greatest secret agent so secret in fact that his codename has codename he is protagonist of british animated television series of same name which was produced by cosgrove hall films for thames television he was voiced by david jason in original series which ran from 1981 to 1992 alexander armstrong in revived series in 2015 contents show biography danger mouse also called dm by colonel k chief by his assistant penfold is antromorphised mouse is known through out series as greatest secret agent in world secret agent so secret that even his codename has codename he wears white body suit with titanium alloy red badge which saved him on couple of occasions with initials dm in yellow he also wears black eyepatch over his left eye it revealed that he only wears it because it is part of his suit especially when he wears it on wrong eye in one particular episode in 2015 revival series he is given eyepatch like gadget called ipatch dms headquarters are in london where he lives in red royal mail post box with his hamster assistant ernest penfold who frequently annoys him with his constant bumbling misinterpreting of instructions dm pilots their modes of transportation mark iii car with wings space hopper space rocket which remarkably both have steering wheels throughout series danger mouse along with penfold battle many villains mostly baron silas greenback personality in series he is shown to very agile expert gymnast which is shown on one episode when he balances on his finger he also knows ancient martial art of kung moggy he often says penfold shush when his assistant is talking too much just plain nonsense good grief in dangerous situation he is brave courageous good hearted always willing to lend hand in fact in one of episodes narrator says he can get you out of jam even marmalade come to that he likes making joke puns knows over thirty four differant languages whenever colonel k contacts him with new mission hes on it straight away urging penfold to come with him despite his assistants many excuses no matter what situation danger mouse is always ready to help out gallery\n",
      "similarity: 0.34950275924736535\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e8'></a>\n",
    "#### Exercise 8: Analyzing and improving BOW search results\n",
    "\n",
    "Experiment with different queries (taking into account the nature of the dataset and your insights from the analysis so far).\n",
    "Answer the following questions:\n",
    "- Does the search perform well? When does it fail? Discuss several examples that are we get an expected but also unexpected results (find at least 3 from each category). Provide reasons for the good/bad result in each case (e.g. is there some error in the data, is there some linguistic phenomenon that we don't capture, is something wrong with our modeling, ...)\n",
    "- If you see problems with search, how could you improve your implementation? Change the functions above, if you think there is room for improvement. Describe your changes and how they made the search better or (in case you made no changes) explain what made the search robust enough to work well."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:58:49.604918226Z",
     "start_time": "2026-02-11T12:58:37.406893536Z"
    }
   },
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "# EXPECTED RESULTS:\n",
    "query = 'Who was the 40th president of the United States?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "#for idx in top_indices:\n",
    "    #print(answers_dataset['validation'][idx]['answer'])\n",
    "    #print(f'similarity: {query_similarity[idx]}')\n",
    "    #print()\n",
    "\n",
    "# returns documents about the US government and various party representatives, as expected. Doesn't return a document containing the answer (Ronald Reagan), but instead the top result names George W Bush as president (who was infact the 43rd president, not 40th). However, the result is surprisingly close enough.\n",
    "\n",
    "query = 'What is the tallest mountain in the world?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "#for idx in top_indices:\n",
    "    #print(answers_dataset['validation'][idx]['answer'])\n",
    "    #print(f'similarity: {query_similarity[idx]}')\n",
    "    #print()\n",
    "\n",
    "# returns documents with trivia Q and A with several mountain related questions, as expected. The top result contains more mentions of mountains like \"what is caucasus mountains what mountain range separates europe western asia between black caspian seas\" and \"what is name of long mountain range located on italian peninsula\", but none of which contain the correct asnwer (Mount Everest). In the top result the questions are mostly about local tallest mountains, while interestingly enough the second top result does contain a worldwide mountain mention, but is about longest mountain range rather than tallest single mountain: \"what is longest mountain range in world answer andes mountains\". Again, this results are surprisingly close enough to the expected result.\n",
    "\n",
    "query = 'Who is the writer of the book series \\'Harry Potter\\'?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "#for idx in top_indices:\n",
    "    #print(answers_dataset['validation'][idx]['answer'])\n",
    "    #print(f'similarity: {query_similarity[idx]}')\n",
    "    #print()\n",
    "\n",
    "# this was the most successful information retrieval so far. The top result is a text discussing general infromation about the book series 'Harry Potter', and includes the name of its author \"j k rowling originally intended to call book harry potter\" (J.K Rowling) as expected.\n",
    "\n",
    "# UNEXPECTED RESULTS\n",
    "query = 'What is the world population?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "#for idx in top_indices:\n",
    "    #print(answers_dataset['validation'][idx]['answer'])\n",
    "    #print(f'similarity: {query_similarity[idx]}')\n",
    "    #print()\n",
    "\n",
    "# this query produces a completely unexpected output because the top result doesn't contain any information aboyt population for any region, or world population (as expected). The top result is a trivia Q and A about various countries in the world but none of them reference population. The second top result is a text with information on mediterranean and african geography. I wonder why these are the top results since, as far as I can tell, none reference population. It could be because they both contain many names of countries, and regions, but I doubt this will have a high cosine similarity with the word \"world\", as we haven't used semantic word embedding.\n",
    "\n",
    "query = 'Who was the first president of Spain?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "#for idx in top_indices:\n",
    "   # print(answers_dataset['validation'][idx]['answer'])\n",
    "    #print(f'similarity: {query_similarity[idx]}')\n",
    "    #print()\n",
    "\n",
    "# this query doesn't produce the expected result (Adolfo Su√°rez), the top result instead is about the first president of the US. This is likely because of lack of information in the dataset, is likely that most infromation in the dataset is based on American sources, or other english-speaking sources. Additionally, the top result list the first president as John Hanson, which is not true in the modern sense of the word. Nowadays presidents of the US are counted in the context of the US constitution, as the first president under the contrituion was Gearge Washigton. This highlights the importanc eof context when answering performing infromation retrieval.\n",
    "\n",
    "query = 'What is the capital of Russia?'\n",
    "embedded_query = embed_text(query, clean, tokenize, lambda x: bag_of_words(x, token_to_id))\n",
    "query_similarity = cosine_similarity_1_to_n(embedded_query, valid_answers_bows)\n",
    "top_indices = top_k_indices(query_similarity, k=5).tolist()\n",
    "for idx in top_indices:\n",
    "    print(answers_dataset['validation'][idx]['answer'])\n",
    "    print(f'similarity: {query_similarity[idx]}')\n",
    "    print()\n",
    "\n",
    "# this query also doesn't produce the expected result (Moscow). The top results are trivia Q and As, where th top result does mention russia \"what mountains divide european part of russia from asian part\" but is not about the capital Moscow. Other questions on these trivia Q and As contain other geography questions, none of which are relevant to the query. The second top result is text discussing mediterranean and african geography, but not about Russia. This is surprising because we haven't used semantic embedding so there's no reason to believe that geography-related questions will return other geography-related texts when the text doesn't contain the target words \"russia\" or \"capital\".\n",
    "### YOUR CODE ENDS HERE"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europe jeopardy template which of contiguous forty eight states has largest land area 100 what is columbia what is city located on conagree river that is capital of south carolina 100 what is arctic ocean northern part of russia borders what ocean 100 in what mountain range is country of switzerland found 100 what is northern hemisphere eastern hemisphere in what hemispheres is russia located 200 what is ural mountains what mountains divide european part of russia from asian part 200 vienna is capital of what european country 200 what is north sea what sea touches shores of england scotland netherlands denmark norway 200 in what direction are you traveling when driving from budapest to warsaw 200 which country is farther south portugal spain 300 what is name of long mountain range located on italian peninsula 300 does prime meridian pass through north south pole 300 which u s city is farthest north los angeles san diego san francisco 300 what country has land border with denmark 300 what is united kingdom island of bermuda is colony of what nation 400 what is caucasus mountains what mountain range separates europe western asia between black caspian seas 400 what is vatican city monaco name two smallest countries in world both located in europe both less than one square mile in size 400 what is asia europe north america what three continents does arctic circle pass through 400 madrid is to spain as what is to france 400 what is ankara turkey what is capital of only country that borders both mediterranean sea black sea 500 what is sweden finland russia estonia latvia lithuania poland germany denmark what countries border baltic sea 500 who owns azores islands 500 is most of turkey located in europe asia 500\n",
      "similarity: 0.6963536896993098\n",
      "\n",
      "africa europe history with rubinstein at springer elementary school studyblue which sea is directly north of africa mediterraneansea what large island is east of mozambique madagascar which strait lies between morocco spain strait of gibraltar what cape is on southwestern tip of africa cape of good hope what african country border mediterranean sea red sea egypt what is capital of africas large island country to east antananarivo what is africas southernmost country south africa what african country is closest to italy tunisia what large african lake does equator cross lake victoria what country shares borderss with egypt ethiopia sudan what channel is west of madagascar mozambique what lake in ghana does prime meridian cross lake volta what is africas westernmost capital dakar what is national capital of egypt cairo what is national capital of congo kinshasa what is national capital of nigeria abuja what is national capital of algeria algiers what is national capital of somalia mogadishu what is national capital of ethiopia addis ababa what mountain is shown in elephant picture mt killmanjaro what river is in picture of victoria falls zambize river do african rivers begin in coast end at coast african rivers end at coast true false hottest temperature on record 136 4 f was taken in death valley false sahara is larger than 48 contiguous united states true sahara is in southern africa false sahara means desert in arabic true sahara is made up of entirely of sand false sahara is largest desert in world true what year did african countries begin winning theri independene 1941 today what is only territory in africa western sahara in many places in africa what is only common thing from colonial power language which country has about seven times number of people per car as senegal south africa what sea separates europe from africa mediterraneansea there are three peninsulas in southern europe balkan peninsula is west of black sea country of italy is peninsula what is name of third peninsula which is bordered by mediterranean sea atlantic ocean bay of biscay ibererian peninsula what are two mountain ranges that separates europe from asia caucus urals mountain ranges which mountain range is south central europe lies just north of italian peninsula apenines which is correct land cover for regions for black sea lowland ukraine cropland in italy ___river flows from alps into adriatic sea po river west of alps _____ river flows into gulf of lion rhone river in great briian ____ river flows through london thames river what country borders spain to west portugual what is island west of united kingdom ireland what country is south of macedonia greece what country shares borders with both poland france germany what island country shares borders with both belarus romania ukrane what country lies between crotia serbia monterago what country borders latvia to north estonia what country borders bulgaria is also partly in aisa turkey which three countries share vast northern peninsula with russia norway sweden finland which three countries are in both europe asia russia\n",
      "similarity: 0.6852691451990393\n",
      "\n",
      "10000_questions 35 10000 general knowledge questions answers 10000_questions 35 10000_questions 35 10000 general knowledge questions school view full document 10000 general knowledge questions answers www cartiaz ro no questions quiz 17 answers 51 what is samsoe type of cheese 52 fylfot is heraldic name for what symbol swastika 53 where would you find howdah back of elephant basket 54 in what country is language fanti spoken ghana 55 what flowers name translates from greek as water vessel hydrangea 56 which of henry eights wives was widow of elder brother catherine of aragon 57 boys from syracuse is based on what shakespeare play comedy of errors 58 hathor was egyptian goddess of what sky 59 larva of click beetle is called what wireworm 60 in australian slang what is ten ounce sandwich liquid lunch can of beer 61 what is name of largest moon of jupiter ganymede 62 mason dixon line separates pennsylvania what state maryland 63 kinkajou belongs to what family of animals raccoon 64 what is hindu kush mountain range 65 caligari is capital of what island sardinia this is end of preview sign up to access rest of document term 10000 general knowledge questions answers www cartiaz ro no questions quiz 19 ans 10000_questions 38\n",
      "similarity: 0.6360198712271061\n",
      "\n",
      "what is capital of florida capital of com dates of religious civil holidays around world www when is com capital of florida capital city of florida is city of tallahassee population of tallahassee was 181412 367413 in metropolitan area florida is one of states in united states of america additional information\n",
      "similarity: 0.6305124996936529\n",
      "\n",
      "what is capital of somalia capital of com dates of religious civil holidays around world www when is com capital of somalia capital city of somalia is city of mogadishu population of mogadishu in year 2008 was 9558666 somalia is somali arabic speaking country on coasts of indian ocean arabian sea additional information\n",
      "similarity: 0.6291982286679644\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "EXPECTED RESULTS:\n",
    "\n",
    "QUERY 1: Who was the 40th president of the United States?\n",
    "\n",
    "returns documents about the US government and various party representatives, as expected. Doesn't return a document containing the answer (Ronald Reagan), but instead the top result names George W Bush as president (who was infact the 43rd president, not 40th). However, the result is surprisingly close enough.\n",
    "\n",
    "QUERY 2: What is the tallest mountain in the world?\n",
    "\n",
    "returns documents with trivia Q and A with several mountain related questions, as expected. The top result contains more mentions of mountains like \"what is caucasus mountains what mountain range separates europe western asia between black caspian seas\" and \"what is name of long mountain range located on italian peninsula\", but none of which contain the correct asnwer (Mount Everest). In the top result the questions are mostly about local tallest mountains, while interestingly enough the second top result does contain a worldwide mountain mention, but is about longest mountain range rather than tallest single mountain: \"what is longest mountain range in world answer andes mountains\". Again, this results are surprisingly close enough to the expected result.\n",
    "\n",
    "QUERY 3: Who is the writer of the book series 'Harry Potter'?\n",
    "\n",
    "this was the most successful information retrieval so far. The top result is a text discussing general infromation about the book series 'Harry Potter', and includes the name of its author \"j k rowling originally intended to call book harry potter\" (J.K Rowling) as expected.\n",
    "\n",
    "UNEXPECTED RESULTS:\n",
    "\n",
    "QUERY 4: What is the world population?\n",
    "\n",
    "this query produces a completely unexpected output because the top result doesn't contain any information aboyt population for any region, or world population (as expected). The top result is a trivia Q and A about various countries in the world but none of them reference population. The second top result is a text with information on mediterranean and african geography. I wonder why these are the top results since, as far as I can tell, none reference population. It could be because they both contain many names of countries, and regions, but I doubt this will have a high cosine similarity with the word \"world\", as we haven't used semantic word embedding.\n",
    "\n",
    "QUERY 5: Who was the first president of Spain?\n",
    "\n",
    "this query doesn't produce the expected result (Adolfo Su√°rez), the top result instead is about the first president of the US. This is likely because of lack of information in the dataset, is likely that most infromation in the dataset is based on American sources, or other english-speaking sources. Additionally, the top result list the first president as John Hanson, which is not true in the modern sense of the word. Nowadays presidents of the US are counted in the context of the US constitution, as the first president under the contrituion was Gearge Washigton. This highlights the importanc eof context when answering performing infromation retrieval.\n",
    "\n",
    "QUERY 6: What is the capital of Russia?\n",
    "\n",
    "this query also doesn't produce the expected result (Moscow). The top results are trivia Q and As, where th top result does mention russia \"what mountains divide european part of russia from asian part\" but is not about the capital Moscow. Other questions on these trivia Q and As contain other geography questions, none of which are relevant to the query. The second top result is text discussing mediterranean and african geography, but not about Russia. This is surprising because we haven't used semantic embedding so there's no reason to believe that geography-related questions will return other geography-related texts when the text doesn't contain the target words \"russia\" or \"capital\".\n",
    "\n",
    "CHANGES PERFORMED TO THE CODE TO IMPROVE THE SEARCH:\n",
    " eliminated punctuation in the clean function, which made the search more robust to different forms of the same word (e.g. \"mountain\" and \"mountain.\" are now treated as the same word). This change improved the search results for queries that contained punctuation, as it allowed us to match the words in the query with the words in the answers more effectively. Additionally, it eliminated the punctuation tokens like '.' or ',' from the vocabulary, which were not informative for the search and could have introduced noise in the similarity calculations. Overall, this change made the search more robust and improved the quality of the retrieved answers. This ensures that similarity is calculated based on keywords with meaning, instead of being influenced by the naturally abundant punctuation tokens. We also changed the order in which we apply the elimination of double spacing, because we noticed that some of the cleaning steps (like eliminating punctuation) can introduce double spacing, after we had applied the double spasing elimination. So we changed the order in which we execute each clean to ensure that double spacing is eliminated at the end of the cleaning process, which made the cleaning process more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "\n",
    "In this section we will implement the TF-IDF algorithm. While BOW is a simple way to represent the documents, it has some limitations. For example, it does not take into account the importance of each word in the document. TF-IDF representation takes into account the frequency of each word in the document and the frequency of the word in the whole dataset. It is a widely used technique in information retrieval and text mining. Refer to the lecture slides for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "<a name='e9'></a>\n",
    "#### Exercise 9: Inverse Document Frequency (IDF)\n",
    "In this exercise, you will implement the TF-IDF algorithm. First, calculate Inverse Document Frequency (IDF) for each word in the vocabulary. Intuitively, it is a measure of how informative a word is based on the whole dataset. Consult the lecture slides for the details. The IDF is calculated as follows:\n",
    "$$\n",
    "IDF(t) = log_{10}(N/df(t))$$\n",
    "where $N$ is the total number of documents (sentences) in the dataset and $df(t)$ is the number of documents containing the word $t$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:30:46.989027444Z",
     "start_time": "2026-02-11T12:30:46.914194901Z"
    }
   },
   "source": [
    "def calculate_idf(bows):\n",
    "    \"\"\"\n",
    "    Calculates the IDF for each word in the vocabulary\n",
    "    Args:\n",
    "        bows: numpty array of size (N x D) where N is the number of documents and D is the vocabulary size\n",
    "\n",
    "    Returns: a numpy array of size D with IDF values for each token\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To avoid the data leakage, the IDF should be calculated the train subset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2026-02-11T12:36:33.452595875Z",
     "start_time": "2026-02-11T12:30:46.997286553Z"
    }
   },
   "source": [
    "train_answers_bows = []\n",
    "for example in tqdm.tqdm(answers_dataset['train']):\n",
    "    train_answers_bows.append(bag_of_words(example['answer_tokens'], token_to_id))\n",
    "\n",
    "train_answers_bows = np.array(train_answers_bows)\n",
    "\n",
    "idf = calculate_idf(train_answers_bows)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 4837/47916 [05:46<51:24, 13.97it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[35]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m train_answers_bows = []\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m example \u001B[38;5;129;01min\u001B[39;00m tqdm.tqdm(answers_dataset[\u001B[33m'\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m'\u001B[39m]):\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     train_answers_bows.append(\u001B[43mbag_of_words\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43manswer_tokens\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken_to_id\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m      5\u001B[39m train_answers_bows = np.array(train_answers_bows)\n\u001B[32m      7\u001B[39m idf = calculate_idf(train_answers_bows)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36mbag_of_words\u001B[39m\u001B[34m(sentence_tokens, token_to_id)\u001B[39m\n\u001B[32m     11\u001B[39m bow = np.zeros(vocab_size, dtype=\u001B[38;5;28mint\u001B[39m)\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m### YOUR CODE HERE\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m bow = np.array([sentence_tokens.count(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m vocab])\n\u001B[32m     16\u001B[39m \u001B[38;5;66;03m### YOUR CODE ENDS HERE\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m bow\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full TF-IDF\n",
    "\n",
    "<a name='e10'></a>\n",
    "#### Exercise 10: TF-IDF\n",
    "- Calculate TF-IDF on the `test` subset of the dataset.\n",
    "- Analyze the search results based on your implemented TF-IDF. Does the search perform well? When does it fail? Discuss several examples that are we get an expected but also unexpected results (find at least 3 from each category). Provide reasons for the good/bad result in each case (e.g. is there some error in the data, is there some linguistic phenomenon that we don't capture, is something wrong with our modeling with average embeddings, ...)\n",
    "- Compare the results with the ones you got with the bag-of-words representation. Discuss the differences and similarities. Do you think TF-IDF is a better representation for this task? Why or why not? Provide examples to support your arguments."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# You can implement the following functions, but you can also use your own design.\n",
    "\n",
    "def calculate_tf_idf_n(bows, idf):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF for each word in the vocabulary\n",
    "    Args:\n",
    "        bows: numpty array of size (N x D) where N is the number of documents and D is the vocabulary size\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size (N x D) with TF-IDF values for each document and each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def calculate_tf_idf(bow, idf):\n",
    "    \"\"\"\n",
    "    Calculates the TF-IDF for a single document\n",
    "    Args:\n",
    "        bow: a numpy array of size D with the bag-of-words representation of the document\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size D with TF-IDF values for each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def embed_tf_idf(sentence, token_to_id, idf):\n",
    "    \"\"\"\n",
    "    Embeds the sentence using TF-IDF\n",
    "    Args:\n",
    "        sentence: a list of tokens\n",
    "        token_to_id: a dictionary mapping each word to an index in the vocabulary\n",
    "        idf: a numpy array of size D with IDF values for each token\n",
    "\n",
    "    Returns: a numpy array of size D with TF-IDF values for each token\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# query = dataset['validation'][0]['query']\n",
    "# print('query:', query)\n",
    "# query_tfidf = embed_text(query, clean, tokenize, lambda x: embed_tf_idf(x, token_to_id, idf))\n",
    "#\n",
    "# answers_tfidf = calculate_tf_idf_n(valid_answers_bows, idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word Embeddings\n",
    "\n",
    "\n",
    "Word embeddings are a powerful model for representing words and their meaning (in terms of distributional similarity). As we discussed in class, we can use them in a wide variety of tasks with more complex architectures. Word vectors offer a dense vector for each word.\n",
    "\n",
    "In this section you will load the pre-trained word embeddings model - Glove. You can read more about it [here](https://aclanthology.org/D14-1162/) ([https://aclanthology.org/D14-1162/](https://aclanthology.org/D14-1162/)). The embeddings are trained on a large corpus of text and are available in different dimensions. We will start with the dimension of 100, but later you will be asked to experiment with other dimensions.\n",
    "\n",
    "You can download the embeddings manually from one of the following links:\n",
    "- https://www.kaggle.com/datasets/pkugoodspeed/nlpword2vecembeddingspretrained/data?select=glove.6B.50d.txt\n",
    "- https://github.com/nishankmahore/word2vec-flask-api (check the table in the readme)\n",
    "\n",
    "The extracted files should contain several models but for now we will be using the 50-dimensional one. This means that each token is represented as a 50-dimensional floating point vector."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "glove_embeddings_path = 'glove.6B/glove.6B.50d.txt'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will load and parse this file line-by-line. Your job in the next exercise is the parsing part.\n",
    "\n",
    "<a name='e11'></a>\n",
    "#### Exercise 11: Parsing the embeddings\n",
    "Implement the following function to parse a single line of the glove embeddings file. The line contains string values separated by spaces. The first value is the token and the rest are the elements of the embedding vector (the values should be cast to float). You can inspect the file to get a better idea of what is there. Return both the token (as string) and the embedding (as a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def parse_embeddings_row(line):\n",
    "    \"\"\"\n",
    "    Parses a single line from the GloVe embeddings file. The line contains a word followed by its embedding values separated by spaces.\n",
    "    Args:\n",
    "        line: a line from the GloVe embeddings file\n",
    "\n",
    "    Returns: a tuple (word, embedding) where 'word' is a string and 'embedding' is a numpy array of floats\n",
    "    \"\"\"\n",
    "    line_split = line.split()\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return token, embedding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we load the file and iterate over the lines to load the tokens and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "tokens = []\n",
    "embeddings = []\n",
    "with open(glove_embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        token, embedding = parse_embeddings_row(line)\n",
    "        tokens.append(token)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.stack(embeddings, axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will create a class `WordEmbeddings` that will hold embeddings and expose useful methods to embed a single token (`embed_token()`) and the whole sentence (`embed_sentence()`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sentence Embeddings by Averaging Word Embeddings\n",
    "In the course, we will see different architectures that take into account the sequence of words (by combining their vectors). A first naive but simple and sometimes (as we are going to see) quite effective approach would be to represent a sentence with an embedding vector that is the average of the word vectors that form the sentence.\n",
    "\n",
    "So formally, this is what we are aiming for:\n",
    "\n",
    "$\n",
    "\\text{Sentence_Embedding} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{Word_Embedding}_i\n",
    "$\n",
    "\n",
    "where:\n",
    "* $N$ is the number of words in a sentence\n",
    "* $\\text{Word_Embedding}_i$ is the word vector for the $i$-th in the sentence.\n",
    "\n",
    "Things to note:\n",
    "* The embedding vector for the sentence will obviously have the same dimension as the word embedding.\n",
    "* This representation ignores the word order (like bag-of-words). During the course we will see how we can overcome this limitation by using sequence models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e12'></a>\n",
    "#### Exercise 12: Word Embeddings Class\n",
    "Implement the two methods in the following class:\n",
    "- `embed_token()` - accepts a token to be embedded. Use `self.token_to_id` to find the row in the `self.embeddings` attribute. If the token is outside the vocabulary, return `None`.\n",
    "- `embed_sentence()` - accepts a list of tokens that form a sentence. Each token (that is inside the vocabulary) should be embedded. The second parameter `reduction` will determine how the embedded tokens are \"reduced\". There are three options: `mean` should average the tokens (resulting in a single numpy array of size `embedding_dim`, which is 50 for our model), `sum` should sum the tokens, and `none` should return the embeddings of all tokens as a single numpy array (the array should be of shape `(N, embedding_dim)`). Think about a situation where none of the tokens in a sentence is in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class WordEmbeddings:\n",
    "    def __init__(self, vocabulary, embeddings):\n",
    "        \"\"\"\n",
    "        Initializes the WordEmbeddings object.\n",
    "        Args:\n",
    "            vocabulary: list of str\n",
    "            embeddings: np.ndarray of shape (vocab_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        self.token_to_id = {token: i for i, token in enumerate(vocabulary)}\n",
    "        self.id_to_token = {i: token for i, token in enumerate(vocabulary)}\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def embed_token(self, token):\n",
    "        \"\"\"\n",
    "        Embed a single token into its word embedding.\n",
    "        Args:\n",
    "            token: str, the token to embed\n",
    "\n",
    "        Returns: np.ndarray with the embedded token or None if the token is not in the vocabulary\n",
    "        \"\"\"\n",
    "        embedding = None\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return embedding\n",
    "\n",
    "    def embed_sentence(self, tokens, reduction='none'):\n",
    "        \"\"\"\n",
    "        Embed a sentence (list of tokens) into word embeddings. Reduction can be 'none', 'mean', or 'sum'.\n",
    "        If 'reduction' is 'none', returns a 2D array of shape (len(tokens), embedding_dim). If 'mean' or 'sum',\n",
    "        returns a 1D array of shape (embedding_dim,) with the values averaged or summed respectively.\n",
    "        Args:\n",
    "            tokens: list of str\n",
    "            reduction: str, one of 'none', 'mean', 'sum'\n",
    "\n",
    "        Returns: np.ndarray with the embedded sentence\n",
    "        \"\"\"\n",
    "        embedding = None\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return embedding"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "glove50_model = WordEmbeddings(tokens, embeddings)\n",
    "del tokens, embeddings"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's test the method `embed_sentence()`. Notice the shape of the returned arrays."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "embedding = glove50_model.embed_sentence('how are you doing ?'.split(' '))\n",
    "print(embedding.shape)\n",
    "print(embedding[:, :10])\n",
    "embedding = glove50_model.embed_sentence('how are you doing ?'.split(' '), reduction='mean')\n",
    "print(embedding.shape)\n",
    "print(embedding[:10])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Similarities between words\n",
    "\n",
    "The function below returns the most similar words to the word provided. The returned list does not contain the word itself."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def most_similar_words(word, model:WordEmbeddings, k):\n",
    "    \"\"\"\n",
    "    Finds the k most similar words to the given word using cosine similarity.\n",
    "    The returned list should contain tuples (word, similarity) sorted by similarity (descending from the most similar).\n",
    "    Args:\n",
    "        word: str, the word to find similar words for\n",
    "        model: WordEmbeddings, the word embeddings model\n",
    "        k: int, the number of similar words to return\n",
    "\n",
    "    Returns: list of tuples (word, similarity)\n",
    "    \"\"\"\n",
    "    embedding = model.embed_token(word)\n",
    "    if embedding is None:\n",
    "        return []\n",
    "\n",
    "    word_id = model.token_to_id[word]\n",
    "    all_embeddings = model.embeddings\n",
    "    similarity = cosine_similarity_1_to_n(embedding, all_embeddings)\n",
    "    top_indices = top_k_indices(similarity, k=k + 1).tolist()\n",
    "    most_similar = []\n",
    "    for id in top_indices:\n",
    "        if id == word_id:\n",
    "            continue\n",
    "        most_similar.append((model.id_to_token[id], similarity[id].item()))\n",
    "    return most_similar"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print(most_similar_words('what', glove50_model, k=10))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next function contains the code to plot a similarity matrix between multiple words (e.g. if we want to compare 10 words and their pair-wise similarities). It requires a matrix with similarities (as input) and labels (aka the words) to display in the final figure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def plot_similarity_matrix(matrix, labels):\n",
    "    \"\"\"\n",
    "    Displays a plot of the `matrix` of size (N x N) with the labels specified as a list of size N\n",
    "    Args:\n",
    "        matrix: a square-sized (N x N) numpy array\n",
    "        labels: a list of strings of hte size N\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(matrix)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(labels)), labels=labels)\n",
    "    ax.set_yticks(np.arange(len(labels)), labels=labels)\n",
    "\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            text = ax.text(j, i, f'{matrix[i, j]:.2f}',\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e13'></a>\n",
    "#### Exercise 13: Plotting similarities between words\n",
    "\n",
    "In the following, we will explore some properties of word embeddings through some examples. We will use 6 example words for this purpose but experiment with other set of words as well. Fill in the next cell to create a similarity matrix between a list of words.\n",
    "\n",
    "Experiment with different words and their similarities plotted. Try at least 2 more (different) sets of words of at least 6 words each. Use the `plot_similarity_matrix` function to visualize the results.\n",
    "Comment on the results. Do they make sense? Why some words are closer to each other than others? What does it mean?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "list_of_words = ['love', 'hate', 'life', 'equal', 'alive', 'dead']\n",
    "\n",
    "similarity_matrix = np.zeros((len(list_of_words), len(list_of_words)), dtype=float)\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "plot_similarity_matrix(similarity_matrix, list_of_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Back to Sentence Embeddings\n",
    "\n",
    "Let us go back to embedding the whole sentences by averaging the embeddings in the sentence. Below you can find a code snippet that uses our `embed_text()` function and glove model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "query = 'fox and deer'\n",
    "print(query)\n",
    "\n",
    "query_embedding = embed_text(query, clean, tokenize, lambda x: glove50_model.embed_sentence(x, reduction='mean'))\n",
    "print(query_embedding.shape)\n",
    "print(query_embedding)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e14'></a>\n",
    "#### Exercise 14: Analyze sentence embeddings\n",
    "- Calculate similarity between the word embeddings representations of the selected queries and the dataset sentences.\n",
    "- Analyze the search results. Does the search work as expected? Discuss the results.\n",
    "- Compare the results with the ones you got with the bag-of-words and TF-IDF representation. Discuss the differences and similarities."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Evaluating Retrieval\n",
    "\n",
    "In this last section we will try to evaluate how good our sentence retrieval system is. To keep the computational resources manageable, we will use the test set for that as its size is more manageable.\n",
    "\n",
    "Recall from the lecture in IR that there are several metrics to evaluate retrieval performance by taking into account the relevance of the retrieved results to the query. We will use Recall@K here (for more metrics and more details refer to the lecture slides and the textbooks).\n",
    "\n",
    "Recall@K is a metric used to measure the effectiveness of a search system in retrieving relevant documents within the top $K$ retrieved documents. It calculates the proportion of relevant documents retrieved within the top-$K$ results, compared to the total number of relevant documents in the collection.\n",
    "\n",
    "$\n",
    "\\text{Recall@K} = \\frac{\\text{Number of relevant documents retrieved in the top }-K}{\\text{Total number of relevant documents}}\n",
    "$\n",
    "\n",
    "In our case, we have a sentence, and it's compressed version. To test our system, we will treat compressed sentences as the queries. Each query will have only a single relevant sentence - the corresponding uncompressed sentence.\n",
    "\n",
    "Therefore, for the calculation of Recall@K we will take into account whether the correct retrieved result is contained within the first $K$ retrieved results. For example, if for a query (i.e. a compressed sentence) we retrieve 10 results and within these we see the relevant one (i.e. the full sentence), then Recall@10 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e15'></a>\n",
    "### Exercise 15: Cosine similarity between two sets of vectors\n",
    "\n",
    "In this exercise you will revisit your implementation of the cosine similarity. Generalize it so that it can accept two arrays containing two sets of vectors (first one containing $M$ vectors and the second one $N$ vectors). Compute the cosine similarity between each pair of vectors coming from the two sets. The result should be an array of size $M x N$.\n",
    "\n",
    "Once again, try to write an efficient code. This means no loops. Remember the relation between matrix multiplication and dot product. (Depending on your implementation of the previous function calculating cosine similarity, this one can be almost the same)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def cosine_similarity_m_to_n(vectors, other_vectors):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a multiple vectors and other vectors.\n",
    "    Args:\n",
    "        vectors: a numpy array representing M number of vectors of D dimensions (of the size MxD)\n",
    "        other_vectors: a 2D numpy array representing other vectors (of the size NxD, where N is the number of vectors and D is their dimension)\n",
    "\n",
    "    Returns: a numpy array of cosine similarity between all the vectors and all the other vectors\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following function will use your implementation to calculate Recall@K based on the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def calculate_recall(queries, sentences, labels, k, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Calculates recall@k given the embeddings of the queries and sentences.\n",
    "    Assumes that only a single sentence with the same index as query is relevant.\n",
    "    Batching is implemented to avoid high memory usage.\n",
    "    Args:\n",
    "        queries: a numpy array with the embeddings of N queries\n",
    "        sentences: a numpy array with the embeddings of N sentences available for retrieval\n",
    "        k: number of top results to search for the relevant sentence\n",
    "        batch_size: number of queries to process at a time\n",
    "\n",
    "    Returns: calculated recall@k\n",
    "\n",
    "    \"\"\"\n",
    "    n_queries = queries.shape[0]\n",
    "    correct = np.zeros(n_queries, dtype=bool)\n",
    "\n",
    "    with tqdm.tqdm(total=n_queries) as pbar:\n",
    "        for batch_start in range(0, n_queries, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, n_queries)\n",
    "            queries_batch = queries[batch_start:batch_end]\n",
    "            batch_similarity = cosine_similarity_m_to_n(queries_batch, sentences)\n",
    "\n",
    "            for i, similarity_row in enumerate(batch_similarity):\n",
    "                query_index = batch_start + i\n",
    "                top_k = top_k_indices(similarity_row, k=k, sorted=False)\n",
    "                label = labels[query_index]\n",
    "                if label in top_k:\n",
    "                    correct[query_index] = True\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    recall = np.sum(correct) / n_queries\n",
    "    return recall"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here, we embed both the queries and answers from the validation subset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "query_embeddings = []\n",
    "expected_answers = []\n",
    "for example in tqdm.tqdm(dataset['validation']):\n",
    "    query_tokens = example['query_tokens']\n",
    "    query_embeddings.append(glove50_model.embed_sentence(query_tokens, reduction='mean'))\n",
    "    expected_answers.append(example['answer_id'])\n",
    "query_embeddings = np.stack(query_embeddings, axis=0)\n",
    "expected_answers = np.array(expected_answers)\n",
    "\n",
    "answers_embeddings = []\n",
    "for example in tqdm.tqdm(answers_dataset['validation']):\n",
    "    answer_tokens = example['answer_tokens']\n",
    "    answers_embeddings.append(glove50_model.embed_sentence(answer_tokens, reduction='mean'))\n",
    "answers_embeddings = np.stack(answers_embeddings, axis=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can use the recall function like so:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "recall_at_1 = calculate_recall(query_embeddings, answers_embeddings, expected_answers, k=1, batch_size=1000)\n",
    "print(f'\\n{recall_at_1 * 100:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name='e16'></a>\n",
    "### Exercise 16: Evaluating retrieval methods\n",
    "\n",
    "Calculate recall for different values of $K$ for all methods:\n",
    "- BOW,\n",
    "- TF-IDF,\n",
    "- Pre-trained embeddings.\n",
    "- Another pre-trained embeddings (for example with larger embedding vectors)\n",
    "\n",
    "Make sure to test on the `test` split. Discuss the results. Comment on how recall changes based on the value of $K$. Are the results expected or surprising?\n",
    "\n",
    "The deliverable for this whole lab is a scientific poster and this last question should be the main thing you will include in the poster."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#### YOUR CODE HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--- YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
