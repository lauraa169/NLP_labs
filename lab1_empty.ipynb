{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Group: Labs 19\n",
    "\n",
    "Names:\n",
    "\n",
    "Jesse Visser, Sam Stutterheim, Laura Morales Turrado\n",
    "\n",
    "Student Ids:\n",
    "\n",
    "Sam: i6399526\n",
    "\n",
    "Jesse:\n",
    "\n",
    "Laura: i6384339\n",
    "\n",
    "Use of genAI tools (e.g. chatGPT), websites (e.g. stackoverflow): list websites where you found code (or other info) as well as include information on how you used genAI tools\n",
    "\n",
    "Sources:\n",
    "\n",
    "Mention names of collaborators outside your group\n",
    "\n",
    "Collaborators:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NLP 2026\n",
    "# Lab 1: From Tokens to Language Models\n",
    "\n",
    "As discussed in the first lectures, tokenization is a fundamental step in **Natural Language Processing (NLP)** üß†üí¨ that transforms raw text into structured data for computational models. In this lab, you will explore different **tokenization techniques** üìù, preprocess text data üîç, and implement **tokenization pipelines** using popular NLP libraries üèóÔ∏è. Next, we explore N-gram language models üìä as a foundational approach üß± to modeling natural language statistically üìà. These models form the conceptual foundation üß† of the neural language models ü§ñüß† that we will explore later in the course üöÄüìö.\n",
    "\n",
    "You will also gain **hands-on experience** with **Hugging Face Datasets ü§óüìö**, while assessing the impact of tokenization choices on downstream NLP tasks. \n",
    "\n",
    "By the end of this lab, you will have a **strong foundation** in tokenization techniques and be able to apply them effectively in **real-world NLP applications**, including implementing a N-gram language modeling pipelineüåç.  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Learning Goals**  \n",
    "\n",
    "By the end of this lab, you should be able to:  \n",
    "\n",
    "‚úÖ **Understand the role of tokenization in NLP** üß†üí°  \n",
    "‚úÖ **Explain why tokenization is important** and how it affects text processing üìñüîç  \n",
    "‚úÖ **Implement different tokenization techniques** ‚Äì Apply **word** üìù, **subword** üî¢, and **character-level** üî† tokenization using built-in libraries.  \n",
    "‚úÖ **Use Hugging Face Datasets** ü§óüìä ‚Äì Load and preprocess text datasets efficiently.  \n",
    "‚úÖ **Evaluate tokenization impact** üìâüîé ‚Äì Analyze how different tokenization methods influence model performance. \n",
    "‚úÖ **Identify challenges in tokenization** ‚ùóüîç ‚Äì Recognize issues like **out-of-vocabulary (OOV) words**, **ambiguity**, and **multilingual tokenization** üåç.  \n",
    "‚úÖ **Identify challenges in tokenization** ‚ùóüîç ‚Äì Recognize issues like **out-of-vocabulary (OOV) words**, **ambiguity** and **multilingual tokenization** üåç.  \n",
    "‚úÖ **Build and use N-gram language models** üß†üìä - Understand how unigrams, bigrams and higher-order n-grams capture context in text, implement efficient N-gram counting with Python and design an object-oriented N-gram Language Model that supports probability lookup and text generation.  \n",
    "‚úÖ **Generate, smooth and evaluate language models** üöÄüìâ - Apply different techniques for text generation and compare models with different values of $N$ and advanced techniques such as interpolation or backoff.\n",
    "\n",
    "### Score breakdown\n",
    "\n",
    "| Exercise            | Points |\n",
    "|---------------------|--------|\n",
    "| [Exercise 1](#e1)   | 3      |\n",
    "| [Exercise 2](#e2)   | 6      |\n",
    "| [Exercise 3](#e3)   | 4      |\n",
    "| [Exercise 4](#e4)   | 6      |\n",
    "| [Exercise 5](#e5)   | 4      |\n",
    "| [Exercise 6](#e6)   | 7      |\n",
    "| [Exercise 7](#e7)   | 4      |\n",
    "| [Exercise 8](#e8)   | 4      |\n",
    "| [Exercise 9](#e9)   | 4      |\n",
    "| [Exercise 10](#e10) | 6      |\n",
    "| [Exercise 11](#e11) | 4      |\n",
    "| [Exercise 12](#e12) | 4      |\n",
    "| [Exercise 13](#e13) | 4      |\n",
    "| [Exercise 14](#e14) | 4      |\n",
    "| [Exercise 15](#e15) | 4      |\n",
    "| [Exercise 16](#e16) | 4      |\n",
    "| [Exercise 17](#e17) | 4      |\n",
    "| [Exercise 18](#e18) | 4      |\n",
    "| [Exercise 19](#e19) | 20     |\n",
    "| Total               | 100    |\n",
    "\n",
    "This score will be scaled down to X.X and that will be your final lab score.\n",
    "\n",
    "### üìå **Instructions for Delivery** (üìÖ **Deadline: X/Feb 18:00**, üé≠ *wildcards possible*)  \n",
    "\n",
    "‚úÖ **Submission Requirements**  \n",
    "+ üìÑ You need to submit a **notebook** üìì with the code, appropriate comments and figures in all questions. Make sure to have a mix of code (some explanations needed if not clear what you implement), figures to support the answers or your claims and proper amount of text to explain your reasoning, answer etc.  \n",
    "+ ‚ö° Make sure that **all cells are executed properly** ‚öôÔ∏è and that **all figures/results/plots** üìä you include in the report are also visible in your **executed notebook**.\n",
    "+ You can work on Google Collab (or other environments), but you need to make sure that your delivered notebook is executed properly.\n",
    "\n",
    "‚úÖ **Collaboration & Integrity**  \n",
    "+ üó£Ô∏è While you may **discuss** the lab with others, you must **write your solutions with your group only**. If you **discuss specific tasks** with others, please **include their names** below.  \n",
    "+ üìú **Honor Code applies** to this lab. For more details, check **Syllabus ¬ß7.2** ‚öñÔ∏è.  \n",
    "+ üì¢ **Mandatory Disclosure**:  \n",
    "   - Any **websites** üåê (e.g., **Stack Overflow** üí°) or **other resources** used must be **listed and disclosed**.  \n",
    "   - Any **GenAI tools** ü§ñ (e.g., **ChatGPT**) used must be **explicitly mentioned**.  \n",
    "   - üö® **Failure to disclose these resources is a violation of academic integrity**. See **Syllabus ¬ß7.3** for details.   "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**A note from Jerry on using Language Models (LMs)**\n",
    "\n",
    "If you try hard enough, you will likely get away with cheating (that does not only apply to LMs). Fortunately, our job is not to police, but rather to educate you. So, please consider the following:\n",
    "\n",
    "- We assume that you are taking this course to learn something! LMs are not always right (they often fail in silly ways). This course should prepare you to detect when they are wrong!\n",
    "- We don't restrict the use of LMs because we see the value of being helped when coding (esp. in the context of a course like NLP). Based on what we saw last year in your deliverables, it's pretty clear when you \"copy\" some code and then you struggle to interpret the results. This is the essence of this course and of the skills you should try build for yourself: Many people can run fancy models these days but not many people can interpret the results correctly. Try to be the latter ones."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 0. Preparation\n",
    "\n",
    "In this section we will import the necessary libraries. But first we will explain how to install a python environment using conda. You don't have to do it this way, and maybe you already have one prepared.\n",
    "\n",
    "First, you can install Miniconda environment manager. Here are the instructions https://www.anaconda.com/docs/getting-started/miniconda/install.\n",
    "It can create virtual environments with different python versions and separately installed libraries (useful if  you work on several projects). You can read more about managing environments here: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html\n",
    "You might need a restart of your system to complete the installation.\n",
    "\n",
    "The commands below are commented out so that you don't run them by mistake. You will have to copy them to the terminal of your operating system."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This command creates a new environment called `nlplab2026` (you can change the name if you'd like) and python version `3.12`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# conda create -n nlplab2026 python=3.12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The environment should be installed, but it is not activated yet. To do it you can use the following command:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# conda activate nlplab2026"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The activation of the environment is indicated by the prompt (the name of the environment should be in parentheses), for example: `(nlplab2026) pawelmaka@MacBook ~ % `\n",
    "\n",
    "Now, we are ready to install the required packages using `pip`:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# pip install --user --upgrade jupyter\n",
    "# pip install tqdm\n",
    "# pip install datasets\n",
    "# pip install matplotlib\n",
    "# pip install numpy\n",
    "# pip install pandas"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sometimes, the libraries don't work together well on some systems. This situation could necessitate the installation of a library's particular version. To do so you can use the following command (putting the library and version you need)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# pip install matplotlib==3.10.8 --force-reinstall -v"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "When you are starting jupyter notebook make sure to first activate the environment `nlplab2026`."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Intro to regular expressions\n",
    "\n",
    "In this introduction section, you can practice the use of regular expressions in python. You can find the documentation here: [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html). The main functions of the re module are:\n",
    "- `re.search()` - searches for a pattern in a string, returns the first match,\n",
    "- `re.findall()` - similar to `search()`, but returns a list of all matches,\n",
    "- `re.sub()` - replaces the matches with a string.\n",
    "\n",
    "All above functions accept the regular expression pattern as their argument. The patterns are strings that represent the rules for matching the text. In python they start with `r` character, e.g. `r'\\d'` is a pattern that matches a digit.\n",
    "\n",
    "Let us start with a simple example. We will search for the word \"world\" in the string \"Hello, world!\"."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text = \"Hello, world!\"\n",
    "pattern = r'world'\n",
    "match = re.search(pattern, text)\n",
    "print(match)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `search()` function returns a match object that tells us where the match was found (`span` argument) and the exact part of the string that matched the pattern (`group` argument).\n",
    "\n",
    "Below you can find the examples from the lecture."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Disjunctions\n",
    "pattern = r'[wW]oodchuck'  # matches both \"woodchuck\" and \"Woodchuck\"\n",
    "pattern = r'[1234567890]'  # matches any digit, you can also use r'\\d' or r'[0-9]'\n",
    "pattern = r'[0-9]'  # matches any digit\n",
    "pattern = r'[A-Z]'  # matches any uppercase letter\n",
    "pattern = r'[a-z]'  # matches any lowercase letter\n",
    "pattern = r'[A-Za-z]'  # matches any letter\n",
    "\n",
    "# Disjunctions with pipe |\n",
    "pattern = r'groundhog|Woodchuck'  # matches both \"groundhog\" and \"Woodchuck\"\n",
    "\n",
    "# Negation (only when in [])\n",
    "pattern = r'[^0-9]'  # matches any character that is not a digit\n",
    "pattern = r'[^Ss]'  # matches any character that is not 'S' or 's'\n",
    "pattern = r'a\\^b'  # matches the string \"a^b\"\n",
    "\n",
    "# Quantifiers (+, *, ?, .)\n",
    "pattern = r'baa+'  # matches \"ba\" followed by one or more \"a\" (e.g. \"baa\", \"baaa\", \"baaaa\", ...)\n",
    "pattern = r'oo*h'  # matches \"o\" followed by zero or more \"o\" and then \"h\" (e.g. \"oh\", \"ooh\", \"oooh\", ...)\n",
    "pattern = r'colou?r'  # matches \"color\" and \"colour\"\n",
    "pattern = r'beg.n'  # matches \"begun\", \"begin\", \"begnn\", ...\n",
    "\n",
    "# Anchors (^, $)\n",
    "pattern = r'^Hello'  # matches \"Hello\" at the beginning of the string\n",
    "pattern = r'world!$'  # matches \"world!\" at the end of the string"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Hugging Face Datasets\n",
    "\n",
    "We will be using Hugging Face Datasets library ([https://huggingface.co/datasets](https://huggingface.co/datasets)). You can find the detailed documentation and tutorials here: [https://huggingface.co/docs/datasets/en/index](https://huggingface.co/docs/datasets/en/index)\n",
    "\n",
    "In this lab, we will load ‚Äútweet_eval‚Äù dataset and specifically the __emoji__ subset. For tokenization, we will only consider the text (contents of the tweet) but in this section we will also look at the labels. The description of the dataset is available in the dataset's card at [https://huggingface.co/datasets/cardiffnlp/tweet_eval](https://huggingface.co/datasets/cardiffnlp/tweet_eval)\n",
    "\n",
    "In order to load a dataset simply call load_dataset function specifying the dataset name. You can find many more datasets at the huggingface website."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = datasets.load_dataset('tweet_eval', 'emoji')\n",
    "print(dataset)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The loaded dataset contains three subsets (‚Äútrain‚Äù, ‚Äúvalidation‚Äù, and ‚Äútest‚Äù). Each consists of two columns: ‚Äútext‚Äù and ‚Äúlabel‚Äù. Label is an integer from 0 to 19 representing an emoji. See the dataset's card for more information. We can access the elements of the dataset like so:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(10):\n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can easily cast the dataset to the pandas DataFrame."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tweet_train_df = pd.DataFrame(dataset['train'])\n",
    "print(tweet_train_df)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Even though we will not use the labels in this lab, we can plot their distribution in the training subset."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset's filter function\n",
    "We can filter the examples using ```filter()``` method. See this link for more details https://huggingface.co/docs/datasets/en/use_dataset. Here is an example of filtering the short tweets (less than 20 characters) from the ```train``` subset."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "short_tweets = dataset['train'].filter(lambda example: len(example['text']) < 20)\n",
    "print(short_tweets)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(10):\n",
    "    print(short_tweets[i])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset's map function\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries. We will calculate the length of the text (in characters) in each example."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_text_length(example):\n",
    "    example['text_length'] = len(example['text'])\n",
    "    return example"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = dataset.map(calculate_text_length, desc=\"Calculating text length\")\n",
    "print(dataset)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can plot the histogram of the text lengths."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.DataFrame(dataset['train']).groupby('text_length')['text_length'].count().plot.hist(bins=50)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e1'></a>\n",
    "\n",
    "### Exercise 1: Questions about the datasets\n",
    "1. (1p) What is the size of the training, test and validation datasets?\n",
    "2. (1p) What is the average length (in characters) of the tweets in the training dataset?\n",
    "3. (1p) Compare the distributions of tweet lengths between training, validation, and test subsets."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Q2\n",
    "def getAverage(dataset):\n",
    "    total_length = 0\n",
    "    for instance in dataset['train']:\n",
    "        total_length += len(instance['text'])\n",
    "    return total_length/len(dataset['train'])\n",
    "\n",
    "print(getAverage(dataset))\n",
    "\n",
    "#Q3\n",
    "train_lengths = pd.DataFrame(dataset['train']).groupby('text_length')['text_length'].count()\n",
    "val_lengths = pd.DataFrame(dataset['validation']).groupby('text_length')['text_length'].count()\n",
    "test_lengths = pd.DataFrame(dataset['test']).groupby('text_length')['text_length'].count()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "\n",
    "axes[0].bar(train_lengths.index, train_lengths.values, color='C0')\n",
    "axes[0].set_title('Train: tweet length distribution')\n",
    "axes[0].set_xlabel('Text length (characters)')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].bar(val_lengths.index, val_lengths.values, color='C1')\n",
    "axes[1].set_title('Validation: tweet length distribution')\n",
    "axes[1].set_xlabel('Text length (characters)')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "axes[2].bar(test_lengths.index, test_lengths.values, color='C2')\n",
    "axes[2].set_title('Test: tweet length distribution')\n",
    "axes[2].set_xlabel('Text length (characters)')\n",
    "axes[2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Answers\n",
    "1. Train = 45000, Test = 500000, Validation = 5000\n",
    "2. Average tweet length in the train dataset is 71.01691111111111 characters.\n",
    "3. As we can see in the plots for the character length distribution of the train set, validation set and test set, the distributions are pretty much the same. This is most likely because the splitting of the dataset was stratified by character length to ensure that all subsets have similar distributions."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "In this section we will preprocess the dataset by cleaning and tokenizing the entries.\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Cleaning the text"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e2'></a>\n",
    "\n",
    "### Exercise 2: Write the text cleaning function\n",
    "\n",
    "(6p) Include at least the following steps:\n",
    "1. remove comma between numbers, i.e. 15,000 -> 15000,\n",
    "2. remove multiple spaces,\n",
    "3. space out the punctuation (i.e. \"hello, world.\" -> \"hello , world .\"),\n",
    "4. three more cleaning steps of your choice."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T11:31:00.311329Z",
     "start_time": "2026-01-30T11:31:00.304355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean(text):\n",
    "    \"\"\"\n",
    "    Cleans the text\n",
    "    Args:\n",
    "        text: a string that will be cleaned\n",
    "\n",
    "    Returns: the cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty text\n",
    "    if text == '':\n",
    "        return text\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    text = re.sub(r'(?<=\\d),(?=\\d)', '', text) # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub(r'\\s+', ' ', text) # remove multiple spaces\n",
    "    text = re.sub(r'([.,!?;:\"])', r' \\1 ', text) # space out the punctuation (i.e. \"hello, world.\" -> \"hello , world .\")\n",
    "\n",
    "    text = text.lower() # Extra step: making everything lowercase\n",
    "    text = re.sub(r'http\\S+', '', text) # Extra step: removes urls starting with https\n",
    "    text = re.sub(r'#', '', text) # Extra step: removes hashtags\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return text"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's test your cleaning function"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T11:31:02.439321Z",
     "start_time": "2026-01-30T11:31:02.433205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(5):\n",
    "    original = dataset['train'][i]['text']\n",
    "    cleaned = clean(original)\n",
    "    print('original:', original)\n",
    "    print('cleaned:', cleaned)\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Sunday afternoon walking through Venice in the sun with @user Ô∏è Ô∏è Ô∏è @ Abbot Kinney, Venice\n",
      "cleaned: sunday afternoon walking through venice in the sun with @user Ô∏è Ô∏è Ô∏è @ abbot kinney ,  venice\n",
      "\n",
      "original: Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que)\n",
      "cleaned: time for some bbq and whiskey libations .  chomp ,  belch ,  chomp !  (@ lucille's smokehouse bar-b-que)\n",
      "\n",
      "original: Love love love all these people Ô∏è Ô∏è Ô∏è #friends #bff #celebrate #blessed #sundayfunday @ San‚Ä¶\n",
      "cleaned: love love love all these people Ô∏è Ô∏è Ô∏è friends bff celebrate blessed sundayfunday @ san‚Ä¶\n",
      "\n",
      "original: Ô∏è Ô∏è Ô∏è Ô∏è @ Toys\"R\"Us\n",
      "cleaned: Ô∏è Ô∏è Ô∏è Ô∏è @ toys \" r \" us\n",
      "\n",
      "original: Man these are the funniest kids ever!! That face! #HappyBirthdayBubb @ FLIPnOUT Xtreme\n",
      "cleaned: man these are the funniest kids ever !  !  that face !  happybirthdaybubb @ flipnout xtreme\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's finally use the ```map()``` method and apply your `clean()` function to all entries of the dataset. We are overriding the `text` field with the cleaned version."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_example(example):\n",
    "    \"\"\"\n",
    "    Cleans the example from the Dataset. Uses clean() function for cleaning.\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example containing 'clean' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    clean_text = clean(text)\n",
    "\n",
    "    # Update the example with the cleaned text\n",
    "    example['text'] = clean_text.strip()\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(clean_example, desc=\"Cleaning text\")\n",
    "print(dataset)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Build vocabulary\n",
    "\n",
    "In the previous section, we implemented the cleaning of the dataset. Now, we will tokenize the text splitting it by spaces. We will build a vocabulary based on the cleaned text of the `train` split. We will investigate some properties of corpora (e.g. Zipf's law).\n",
    "\n",
    "The function below builds a vocabulary from the dataset. It counts the occurrences of the words in the dataset using the Counter class. Check the documentation here [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e3'></a>\n",
    "### Exercise 3: Build the vocabulary\n",
    "(4p) Fill in the function below to build the vocabulary from the dataset. The function should return a `Counter` object with the words and their frequencies. The variable named `vocab` is already initialized as an empty `Counter` object."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_vocab_counter(dataset):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from the dataset\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return vocab"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vocab_counter = build_vocab_counter(dataset['train'])\n",
    "print('Size of the vocabulary:', len(vocab_counter))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because we created a counter, we can easily check the most and least common words in the vocabulary. Do the most common words make sense? How about the least common ones?"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('Most common:')\n",
    "print(vocab_counter.most_common(10))\n",
    "print('Least common:')\n",
    "print(vocab_counter.most_common()[-10:])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also plot the counts of the words. You can check the [Power law](https://en.wikipedia.org/wiki/Power_law) if you are more interested."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.loglog([val for word, val in vocab_counter.most_common()])\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The plot shows that the distribution of the words in the vocabulary follows the Zipf's law. The most frequent word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\n",
    "\n",
    "We can also filter the vocabulary by the frequency of the words. We will only consider the most frequent words and mark the rest as the `<unk>` token. Here we set the maximum vocabulary size to 50,000. But you can experiment with different sizes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prune vocabulary\n",
    "max_vocab_size = 50000\n",
    "vocab = vocab_counter.most_common(max_vocab_size)\n",
    "# cast to list of words\n",
    "vocab = [word for word, _ in vocab]\n",
    "print(len(vocab))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e4'></a>\n",
    "### Exercise 4: Frequency of pairs of words\n",
    "Calculate the frequency of (neighbouring) pairs of words in the training dataset.\n",
    "1. (2p) List the most and least common pairs. Do the most common pairs make sense?\n",
    "2. (2p) How many pairs occur only once in the dataset?\n",
    "3. (2p) Plot the distribution of the pair frequencies."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### YOUR CODE HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- YOUR ANSWERS HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will define and add three special tokens to our vocabulary:\n",
    "- unknown token ```<unk>```, which will represent words outside our vocabulary;\n",
    "- begining of sequence token ```<s>```, which will always be the first token of a sequence;\n",
    "- end of sequence token ```</s>```, which will mark the end of the sequence and allow the model to finish generation."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unknown_token = '<unk>'\n",
    "bos_token = '<s>'\n",
    "eos_token = '</s>'\n",
    "vocab.append(unknown_token)\n",
    "vocab.append(bos_token)\n",
    "vocab.append(eos_token)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Tokenize the dataset\n",
    "The function below tokenizes the cleaned text (```example['clean']```) by splitting it on spaces. It replaces the words that are not in the vocabulary with the `<unk>` token."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e5'></a>\n",
    "### Exercise 5: Tokenize the dataset\n",
    "(4p) Fill in the function below to tokenize the dataset. The function will be applied to the dataset through the `map()` method, so it returns the updated example. Your task is to split the text by spaces and replace the words that are not in the vocabulary with the `<unk>` token. The tokenized sequence should start with the `bos_token` token and end with the 'eos_token'."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize(text, vocab, unknown_token='<unk>', bos_token='<s>', eos_token='</s>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the text that is assumed to be cleaned first with the clean() function. The tokenized sequence should start with the `bos_token` token and end with the 'eos_token'.\n",
    "    Args:\n",
    "        text: a cleaned text\n",
    "        vocab: a vocabulary as a list of words\n",
    "        unknown_token: a token to replace the words that are not in the vocabulary\n",
    "    Returns: tokenized text as a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = None  # list of tokens, your code should fill this variable\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's test the function:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenized = tokenize(\"what are you doing someunknowntoken ?\", vocab, unknown_token, bos_token, eos_token)\n",
    "print(tokenized)\n",
    "\n",
    "assert tokenized[0] == bos_token, \"First token should be bos_token\"\n",
    "assert tokenized[-1] == eos_token, \"Last token should be eos_token\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize_example(example, vocab, unknown_token='<unk>', bos_token='<s>', eos_token='</s>'):\n",
    "    \"\"\"\n",
    "    Applies the tokenize() function to the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        vocab: a vocabulary as a list of words\n",
    "        unknown_token: a token to replace the words that are not in the vocabulary\n",
    "    Returns: update example containing 'tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    tokens = tokenize(text, vocab, unknown_token, bos_token, eos_token)\n",
    "    example['tokens'] = tokens\n",
    "    return example\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_example,\n",
    "                      fn_kwargs={\n",
    "                          'vocab': vocab,\n",
    "                          'unknown_token': unknown_token,\n",
    "                          'bos_token': bos_token,\n",
    "                          'eos_token': eos_token,\n",
    "                      }, desc=\"Tokenizing text\")\n",
    "print(dataset)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us examine several entries from the dataset. We can see that the `tokens` column has been added to each example."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(5):\n",
    "    print('Original tweet:')\n",
    "    print(dataset['train'][i]['text'])\n",
    "    print('Tokenized tweet:')\n",
    "    print(dataset['train'][i]['tokens'])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Make sure that the tokenization works as you intended. If not, revisit the cleaning and tokenization functions."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e6'></a>\n",
    "### Exercise 6: Questions about the tokenization\n",
    "1. (1p) How many unknown tokens are in the validation dataset after tokenization?\n",
    "2. (1p) What is the distribution of the number of tokens in the training dataset?\n",
    "3. (1p) How the number of tokens corresponds to the number of characters in our dataset?\n",
    "4. (1p) How the size of the vocabulary (```max_vocab_size```) affects the number of unknown tokens?\n",
    "5. (1p) How does the size of the vocabulary affect the number of tokens in the dataset?\n",
    "6. (1p) Think about the advantages and disadvantages of the tokenization method we used. What are the cases when it will not work well?\n",
    "7. (1p) Decide what vocabulary size you will use for the rest of the lab. It shouldn't be smaller than 10,000. Give a short explanation."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### YOUR CODE HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- YOUR ANSWERS HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. N-grams"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e7'></a>\n",
    "### Exercise 7: Counting unigrams\n",
    "\n",
    "(4p) Fill in the function below. Iterate through the dataset and count the tokens (unigrams). Check the documentation of the `Counter` dictionary (https://docs.python.org/3/library/collections.html#collections.Counter)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_unigrams(dataset):\n",
    "    \"\"\"\n",
    "        Iterates through a dataset to count the frequency of individual tokens (unigrams).\n",
    "\n",
    "        Args:\n",
    "            dataset: a dataset where each example is a dictionary containing a 'tokens' key with\n",
    "                a list of strings.\n",
    "\n",
    "        Returns: a counter object mapping each unique token to its total number of occurrences across the entire dataset.\n",
    "    \"\"\"\n",
    "    unigrams_counter = Counter()\n",
    "    for example in tqdm.tqdm(dataset):\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "    return unigrams_counter\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unigrams = calculate_unigrams(dataset['train'])\n",
    "print(unigrams.most_common(10))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e8'></a>\n",
    "### Exercise 8: Counting n-grams (with n > 1)\n",
    "(4p) Fill in the following function. It should iterate through the dataset and count the n-grams of the order `n` (a parameter)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_ngrams(dataset, n):\n",
    "    \"\"\"\n",
    "    Iterates through the dataset to count the occurrences of n-gram contexts.\n",
    "\n",
    "    Args:\n",
    "        dataset: a dataset object where each example contains a 'tokens' key (a list of strings).\n",
    "        n: the order of the n-gram model.\n",
    "\n",
    "    Returns: a counter object mapping n-gram tuples to their frequency in the dataset.\n",
    "    \"\"\"\n",
    "    ngrams_counter = Counter()\n",
    "    for example in tqdm.tqdm(dataset):\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "    return ngrams_counter"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bigrams = calculate_ngrams(dataset['train'], n=2)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e9'></a>\n",
    "### Exercise 9: Examining n-grams\n",
    "Count the higher-rank n-grams.\n",
    "1. (1p) Check the most common n-grams of each rank.\n",
    "2. (3p) Plot the number of n-grams dependent on the rank n."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### YOUR CODE HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- YOUR ANSWERS HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Having n-grams as Counters is nice, but for generation and calculating perplexity (coming up next) this would be inefficient. For this reason, we will create a class representing language models that are based on n-grams. You can check a quick guide on Object Oriented Programming (OOP) in Python here https://www.geeksforgeeks.org/python/python-oops-concepts/\n",
    "\n",
    "Currently, we have the counts of n-grams. The main problem is that, for generation, we need the counts of each token in the vocabulary GIVEN the previous (n-1) tokens. If we were to leave the n-gram Counter as is, we would have to iterate over each entry and check if the first (n-1) tokens matches what we have already generated. To mitigate this issue, we will create a dictionary of Counters, where keys will be the tuples with (n-1) n-grams. This allows us to simply retrieve the appropriate counter.\n",
    "\n",
    "We are also start using numpy, which is a very potent and prolific library for matrices and their manipulations. It is highly optimized, so it will make our code run faster. You can check the documentation here https://numpy.org/doc/stable/"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e10'></a>\n",
    "### Exercise 10: N-gram Language Model class\n",
    "Fill in the two methods in NgramLanguageModel class:\n",
    " 1. (3p) `build_model()` builds the model, which is a `dictionary` of `Counters`. The keys of this dictionary are the tokens of the n-grams without the last token. The values of the dictionary are separate `Counters` of the final tokens. For example, in case of the tri-grams `(\"at\", \"the\",\"university\")` with count 3 and `(\"at\", \"the\",\"party\")` with count 2, the previous tokens for both is a bi-gram `(\"at\", \"the\")`. The resulting `Counter` for this bi-gram is `{\"university\": 3, \"party\": 2}`.\n",
    " 2. (3p) `get_counts()` returns a numpy array with counts of all vocab tokens given the passed previous tokens. This function operates on the indices of tokens in the vocabulary. The fastest way to get the index of a token is to use the `self.word_to_index` dictionary. It will return the index of the token: `self.word_to_index[token]`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NgramLanguageModel:\n",
    "    \"\"\"\n",
    "    A dictionary-based N-gram Language Model.\n",
    "\n",
    "    Attributes:\n",
    "        vocab: a list of unique tokens in the language.\n",
    "        word_to_index: the apping from token strings to their integer indices.\n",
    "        vocab_size: total number of unique tokens in the vocabulary.\n",
    "        ngrams: a counter of n-gram tuples.\n",
    "        n: the order of the n-gram model\n",
    "        model (dict): a nested dictionary where keys are context tuples and values are Counters of following tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, ngrams, n, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.word_to_index = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.ngrams = ngrams\n",
    "        self.n = n\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Converts flat n-gram counts into a nested dictionary structure for efficiency.\n",
    "\n",
    "        Returns:a dictionary where: key: tuple (the context of length n-1), value: collections.Counter (counts of tokens following that context)\n",
    "        \"\"\"\n",
    "        model = {}\n",
    "        for ngram, count in tqdm.tqdm(self.ngrams.items()):\n",
    "            ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ### YOUR CODE ENDS HERE\n",
    "        return model\n",
    "\n",
    "    def get_counts(self, previous_tokens):\n",
    "        \"\"\"\n",
    "        Retrieves the counts of all vocabulary tokens that follow a given context. Only the last (n-1) tokens are used as the context.\n",
    "\n",
    "        Args:\n",
    "            previous_tokens: the sequence of tokens seen so far.\n",
    "\n",
    "        Returns: an integer np.ndarray of shape (vocab_size,) where the value at index 'i' is the count of vocab[i] following the context.\n",
    "        \"\"\"\n",
    "        previous_tokens = tuple(previous_tokens[-(self.n - 1):])\n",
    "        counts = np.zeros(self.vocab_size, dtype=int)\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        return counts"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let us test this class on a toy example"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "toy_trigrams = Counter({\n",
    "    (\"at\", \"the\", \"university\"): 3,\n",
    "    (\"at\", \"the\", \"party\"): 2,\n",
    "    (\"feed\", \"the\", \"cat\"): 1,\n",
    "    (\"feed\", \"the\", \"dog\"): 4,\n",
    "})\n",
    "\n",
    "toy_vocab = [\"at\", \"the\", \"university\", \"party\", \"feed\", \"cat\", \"dog\"]\n",
    "\n",
    "toy_lm = NgramLanguageModel(toy_trigrams, 3, toy_vocab)\n",
    "print('model', toy_lm.model)\n",
    "\n",
    "counts = toy_lm.get_counts((\"at\", \"the\"))\n",
    "print('counts', counts)\n",
    "\n",
    "assert type(counts) == np.ndarray, \"counts should be a numpy array\"\n",
    "assert counts.size == len(toy_vocab)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `toy_lm.model` should be something like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    ('at', 'the'): Counter({\n",
    "        'university': 3,\n",
    "        'party': 2\n",
    "    }),\n",
    "    ('feed', 'the'): Counter({\n",
    "        'dog': 4,\n",
    "        'cat': 1\n",
    "    })\n",
    "}\n",
    "```\n",
    "\n",
    "The `counts` should be:\n",
    "```\n",
    "[0 0 3 2 0 0 0]\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, we can create our bi-gram language model"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "bigram_lm = NgramLanguageModel(bigrams, 2, vocab)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e11'></a>\n",
    "### Exercise 11: Greedy Generate\n",
    "(4p) Implement the following greedy generation function. It should first call the model object (passed as a parameter) to obtain the counts of each token in our vocabulary given the already generated sequence. Next, it should select the token with the highest count and append it to the already generated sequence. You might find `numpy.argmax()` function handy (https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def greedy_generate(languge_model, max_tokens, bos_token, eos_token, previous_tokens=None):\n",
    "    generated = [bos_token] if previous_tokens is None else previous_tokens\n",
    "    while generated[-1] != eos_token and len(generated) < max_tokens:\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "    return generated"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's test the generations starting with the empty sequence and with \"you can...\"."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generated = greedy_generate(bigram_lm, 15, bos_token, eos_token)\n",
    "print(generated)\n",
    "\n",
    "generated = greedy_generate(bigram_lm, 15, bos_token, eos_token, previous_tokens=[\"<s>\", \"you\", \"can\"])\n",
    "print(generated)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e12'></a>\n",
    "### Exercise 12: Sampling Generate\n",
    "(4p) Implement the sampling generation function. It is similar to the previous one, but the selection of the next tokens should be random, with the probabilities based on the counts returned by the model. Make sure you take care of the case where the n-gram was not encountered (meaning that the model will return all counts equal to zero). Check `numpy.random` package for the appropriate function to sample the tokens with respect to their probabilities (https://numpy.org/doc/2.4/reference/random/legacy.html)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sampling_generate(languge_model, max_tokens, bos_token, eos_token, previous_tokens=None):\n",
    "    generated = [bos_token] if previous_tokens is None else previous_tokens\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's test the function:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generated = sampling_generate(bigram_lm, 15, bos_token, eos_token)\n",
    "print(generated)\n",
    "\n",
    "generated = sampling_generate(bigram_lm, 15, bos_token, eos_token, previous_tokens=[\"<s>\", \"you\", \"can\"])\n",
    "print(generated)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e13'></a>\n",
    "### Exercise 13: Smoothing\n",
    "\n",
    "(4p) In this exercise you will add smoothing to the generation. Smoothing parameter $\\alpha$ is controlled by the parameter `smoothing` with a default value of `1` (too high, as you will see in the next exercises). Consult the lecture slides for the formulation."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sampling_generate_smoothing(language_model, max_tokens, bos_token, eos_token, smoothing=1,\n",
    "                                previous_tokens=None):\n",
    "    generated = [bos_token] if previous_tokens is None else previous_tokens\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return generated"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "generated = sampling_generate_smoothing(bigram_lm, 15, bos_token, eos_token, smoothing=1)\n",
    "print(generated)\n",
    "\n",
    "generated = sampling_generate_smoothing(bigram_lm, 15, bos_token, eos_token, smoothing=1,\n",
    "                                        previous_tokens=[\"<s>\", \"you\", \"can\"])\n",
    "print(generated)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e14'></a>\n",
    "### Exercise 14: Qualitative Analysis of Generations\n",
    "1. (2p) Examine the generations of the functions above (greedy, sampling and sampling with smoothing). Make sure to experiment with different values of smoothing. Generate at least 5 examples for each function and smoothing value.\n",
    "2. (2p) Do the same as above but start with a sequence of tokens (for example `[\"<s>\", \"you\", \"can\"]`) Try at least 3 starting sequences."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### YOUR CODE HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- YOUR ANSWERS HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Perplexity\n",
    "\n",
    "In this section, we will implement and use perplexity as the metric that will help us judge which language model is the best. For this we will first calculate probabilities of each token in a sequence, depending on the previous tokens. For numerical stability, we will actually use logarithms of probabilities."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e15'></a>\n",
    "### Exercise 15: Calculate sequence probabilities\n",
    "\n",
    "(4p) Fill the following function to calculate the logarithms of probabilities of each token in a sequence of tokens. Apply smoothing when calculating the probability. You can first calculate the probability (with smoothing) and then apply `numpy.log()` function."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_total_logprob_smoothing(tokens, language_model, smoothing=0.0):\n",
    "    logprobs = np.zeros(len(tokens), dtype=float)\n",
    "    for i in range(0, len(tokens)):\n",
    "        previous_tokens = tokens[:i]\n",
    "        current_token = tokens[i]\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "    total_logprob = logprobs.sum()\n",
    "    return total_logprob"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e16'></a>\n",
    "### Exercise 16: Calculate sentence perplexity\n",
    "\n",
    "(4p) In this exercise, you will use the function above to calculate the perplexity. Consult the slides for the formulation that depends on the logarithm of probability."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_perplexity(tokens, languge_model, smoothing=0.0):\n",
    "    total_logprob = calculate_total_logprob_smoothing(tokens, languge_model, smoothing)\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return perplexity"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can test our function:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokens = dataset['train'][0]['tokens']\n",
    "print(tokens)\n",
    "\n",
    "perplexity = calculate_perplexity(tokens, bigram_lm, smoothing=1.0)\n",
    "print(perplexity)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e17'></a>\n",
    "### Exercise 17: Perplexity of sentences\n",
    "(4p) Calculate perplexities of several (at least 5) sentences of your choice. Try different values of smoothing. Discuss the results: do the perplexities align with what you expected?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### YOUR CODE HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--- YOUR ANSWERS HERE"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e18'></a>\n",
    "### Exercise 18: Perplexity of the whole dataset\n",
    "\n",
    "(4p) Now we will calculate the perplexity of the entire dataset (or a subset). The function you will implement should calculate the logarithms of probabilities and the lengths of each of the sequences in the dataset. We need both to correctly calculate the perplexity per token."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_perplexity_dataset(dataset, languge_model, smoothing=0.0):\n",
    "    all_logprobs = []\n",
    "    all_lengths = []\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return perplexity\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's calculate the perplexity of the validation dataset."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "validation_perplexity = calculate_perplexity_dataset(dataset['validation'], bigram_lm, smoothing=1.0)\n",
    "print('validation perplexity', validation_perplexity)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Comparing Language Models\n",
    "In this final section, you will use all of the above to evaluate different models against each other. While this is a single exercise, you are expected add as many code and markdown cells as you wish. You can plot the results, add tables, etc."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The models should be evaluated on the `test` subset of the dataset. None of the models should have \"seen\" it during training and any hyperparameter tuning (if applicable). Below, you can see how we can use our function `calculate_perplexity_dataset()` to calculate the perplexity:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_perplexity = calculate_perplexity_dataset(dataset['test'], bigram_lm, smoothing=1.0)\n",
    "print('test perplexity', test_perplexity)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This could take couple of minutes. To test your code you can select a subset of this dataset. But make sure to report the final results in the exercise below on the full test set."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_1000_perplexity = calculate_perplexity_dataset(dataset['test'].select(range(1000)), bigram_lm, smoothing=1.0)\n",
    "print('test (1000 examples) perplexity', test_1000_perplexity)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name='e19'></a>\n",
    "### Exercise 19: More Advanced Language Model\n",
    "(20p) In this exercise, implement either an interpolation or backoff language model that combines more than one n-gram counter.\n",
    "Make sure to adjust weights based on the perplexity of the validation dataset.\n",
    "Compare the tuned model with previous models on the test set. Include n-grams with different values of n.\n",
    "Discuss the results.\n",
    "\n",
    "#### Requirements:\n",
    "- create n-gram language models with higher values of n,\n",
    "- implement at least one more advanced (combining multiple n-grams) language model,\n",
    "- tune its weights on the `validation` subset,\n",
    "- compare the model(s) against the basic models,\n",
    "- discuss the results (Are the results as expected? Why? Why not? etc.)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
